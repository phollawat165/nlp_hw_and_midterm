{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3_pos_tagging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpCKO19dhHPq"
      },
      "source": [
        "# HW 3 - Neural POS Tagger\n",
        "\n",
        "In this exercise, you are going to build a set of deep learning models on part-of-speech (POS) tagging using Tensorflow 2. Tensorflow is a deep learning framwork developed by Google to provide an easier way to use standard layers and networks.\n",
        "\n",
        "To complete this exercise, you will need to build deep learning models for POS tagging in Thai using NECTEC's ORCHID corpus. You will build one model for each of the following type:\n",
        "\n",
        "- Neural POS Tagging with Word Embedding using Fixed / non-Fixed Pretrained weights\n",
        "- Neural POS Tagging with Viterbi / Marginal CRF\n",
        "\n",
        "Pretrained word embeddding are already given for you to use (albeit, a very bad one).\n",
        "\n",
        "We also provide the code for data cleaning, preprocessing and some starter code for tensorflow 2 in this notebook but feel free to modify those parts to suit your needs. Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
        "\n",
        "### Don't forget to change hardware accelrator to GPU in runtime on Google Colab ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx0YbM_8hHPt"
      },
      "source": [
        "## 1. Setup and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF3lPNNkhHPu"
      },
      "source": [
        "We use POS data from [ORCHID corpus](https://www.nectec.or.th/corpus/index.php?league=pm), which is a POS corpus for Thai language.\n",
        "A method used to read the corpus into a list of sentences with (word, POS) pairs have been implemented already. The example usage has shown below.\n",
        "We also create a word vector for unknown word by random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS6O5yT6feRd"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58LvFz30zumq",
        "outputId": "c0ac7c59-5cfe-401d-a7c7-8a7c1a781842"
      },
      "source": [
        "\n",
        "!gdown --id 1tsfqDG8-HL4nkq0pq0HGifND_qDSHq-f\n",
        "!unzip resources.zip "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tsfqDG8-HL4nkq0pq0HGifND_qDSHq-f\n",
            "To: /content/resources.zip\n",
            "153MB [00:02, 70.0MB/s]\n",
            "Archive:  resources.zip\n",
            "   creating: resources/\n",
            "   creating: resources/embeddings/\n",
            "  inflating: resources/embeddings/emb_reader.py  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/resources/\n",
            "   creating: __MACOSX/resources/embeddings/\n",
            "  inflating: __MACOSX/resources/embeddings/._emb_reader.py  \n",
            " extracting: resources/embeddings/__init__.py  \n",
            "  inflating: __MACOSX/resources/embeddings/.___init__.py  \n",
            "   creating: resources/embeddings/__pycache__/\n",
            "  inflating: resources/embeddings/__pycache__/__init__.cpython-36.pyc  \n",
            "   creating: __MACOSX/resources/embeddings/__pycache__/\n",
            "  inflating: __MACOSX/resources/embeddings/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: resources/embeddings/__pycache__/emb_reader.cpython-36.pyc  \n",
            "  inflating: __MACOSX/resources/embeddings/__pycache__/._emb_reader.cpython-36.pyc  \n",
            "  inflating: __MACOSX/resources/embeddings/.___pycache__  \n",
            "  inflating: resources/embeddings/polyglot-th.pkl  \n",
            "  inflating: __MACOSX/resources/embeddings/._polyglot-th.pkl  \n",
            "  inflating: __MACOSX/resources/._embeddings  \n",
            "  inflating: resources/.DS_Store     \n",
            "  inflating: __MACOSX/resources/._.DS_Store  \n",
            "   creating: resources/model/\n",
            "  inflating: resources/model/crf_basic.model  \n",
            "   creating: __MACOSX/resources/model/\n",
            "  inflating: __MACOSX/resources/model/._crf_basic.model  \n",
            "  inflating: resources/model/crf_neural.model  \n",
            "  inflating: __MACOSX/resources/model/._crf_neural.model  \n",
            "  inflating: resources/model/_DS_Store  \n",
            "  inflating: __MACOSX/resources/model/.__DS_Store  \n",
            "  inflating: __MACOSX/resources/._model  \n",
            "  inflating: resources/basic_ff_embedding.pt  \n",
            "  inflating: __MACOSX/resources/._basic_ff_embedding.pt  \n",
            "   creating: resources/data/\n",
            "  inflating: resources/data/orchid_test.txt  \n",
            "   creating: __MACOSX/resources/data/\n",
            "  inflating: __MACOSX/resources/data/._orchid_test.txt  \n",
            "  inflating: resources/data/orchid_train.txt  \n",
            "  inflating: __MACOSX/resources/data/._orchid_train.txt  \n",
            "  inflating: resources/data/orchid97.txt  \n",
            "  inflating: __MACOSX/resources/data/._orchid97.txt  \n",
            " extracting: resources/data/__init__.py  \n",
            "  inflating: __MACOSX/resources/data/.___init__.py  \n",
            "   creating: resources/data/__pycache__/\n",
            "  inflating: resources/data/__pycache__/orchid_corpus.cpython-36.pyc  \n",
            "   creating: __MACOSX/resources/data/__pycache__/\n",
            "  inflating: __MACOSX/resources/data/__pycache__/._orchid_corpus.cpython-36.pyc  \n",
            "  inflating: resources/data/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/resources/data/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/resources/data/.___pycache__  \n",
            "  inflating: resources/data/orchid_corpus.pyc  \n",
            "  inflating: __MACOSX/resources/data/._orchid_corpus.pyc  \n",
            "  inflating: resources/data/__init__.pyc  \n",
            "  inflating: __MACOSX/resources/data/.___init__.pyc  \n",
            "  inflating: resources/data/orchid_corpus.py  \n",
            "  inflating: __MACOSX/resources/data/._orchid_corpus.py  \n",
            "  inflating: __MACOSX/resources/._data  \n",
            "  inflating: __MACOSX/._resources    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2a9b92hYTg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41dd8878-0b34-44d9-fecb-e01e9610a2ed"
      },
      "source": [
        "!pip install python-crfsuite\n",
        "!pip install tensorflow-addons\n",
        "!pip install tf2crf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-crfsuite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\r\u001b[K     |▍                               | 10kB 12.9MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 11.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 8.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |████                            | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 184kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 245kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 266kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 276kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 286kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 296kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 481kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 491kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 501kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 522kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 532kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 542kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 552kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 563kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 573kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 583kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 593kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 716kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 727kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 737kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 747kB 5.7MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7\n",
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.12.1\n",
            "Collecting tf2crf\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f5/e9f972be845a2b0ea93c76d26ed6b7bde599a48f70554a30a528117731c8/tf2crf-0.1.29-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow-addons>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from tf2crf) (0.12.1)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf2crf) (2.4.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.8.2->tf2crf) (2.7.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.1.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.2.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.15.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.32.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.4.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.3.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.12)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=2.1.0->tf2crf) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.27.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (0.4.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.7.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.1.0)\n",
            "Installing collected packages: tf2crf\n",
            "Successfully installed tf2crf-0.1.29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c7upY0fYsdt"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJpCbSvJhHPv"
      },
      "source": [
        "from resources.data.orchid_corpus import get_sentences\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random\n",
        "import tensorflow as tf\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4SZz56ThHP0",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ebe8e8-a97a-4d39-f1dd-207b6f6bb5f5"
      },
      "source": [
        "yunk_emb =np.random.randn(32)\n",
        "train_data = get_sentences('train')\n",
        "test_data = get_sentences('test')\n",
        "print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('การ', 'FIXN'), ('ประชุม', 'VACT'), ('ทาง', 'NCMN'), ('วิชาการ', 'NCMN'), ('<space>', 'PUNC'), ('ครั้ง', 'CFQC'), ('ที่ 1', 'DONM')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awxn_GRIhHP3"
      },
      "source": [
        "Next, we load pretrained weight embedding using pickle. The pretrained weight is a dictionary which map a word to its embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GS3lTZshHP4"
      },
      "source": [
        "import pickle\n",
        "fp = open('resources/basic_ff_embedding.pt', 'rb')\n",
        "embeddings = pickle.load(fp)\n",
        "fp.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CqTNKsChHP7"
      },
      "source": [
        "The given code below generates an indexed dataset(each word is represented by a number) for training and testing data. The index 0 is reserved for padding to help with variable length sequence. (Additionally, You can read more about padding here [https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPGUNEZyhHP8"
      },
      "source": [
        "## 2. Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fMWn8qehHP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae35e7e-02b0-483c-cd29-f9845455d126"
      },
      "source": [
        "word_to_idx ={}\n",
        "idx_to_word ={}\n",
        "label_to_idx = {}\n",
        "for sentence in train_data:\n",
        "    for word,pos in sentence:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = len(word_to_idx)+1\n",
        "            idx_to_word[word_to_idx[word]] = word\n",
        "        if pos not in label_to_idx:\n",
        "            label_to_idx[pos] = len(label_to_idx)+1\n",
        "word_to_idx['UNK'] = len(word_to_idx)\n",
        "\n",
        "n_classes = len(label_to_idx.keys())+1\n",
        "print(n_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgvZ8v_2hHP_"
      },
      "source": [
        "This section is tweaked a little from the demo, word2features will return word index instead of features, and sent2labels will return a sequence of word indices in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktf2KkJghHQA"
      },
      "source": [
        "def word2features(sent, i, emb):\n",
        "    word = sent[i][0]\n",
        "    if word in word_to_idx :\n",
        "        return word_to_idx[word]\n",
        "    else :\n",
        "        return word_to_idx['UNK']\n",
        "\n",
        "def sent2features(sent, emb_dict):\n",
        "    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return numpy.asarray([label_to_idx[label] for (word, label) in sent],dtype='int32')\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [word for (word, label) in sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgBw3I9ShHQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5454d35e-6846-4c37-96aa-2d6411c31698"
      },
      "source": [
        "sent2features(train_data[100], embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 29, 327,   5, 328])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O7oClK-hHQG"
      },
      "source": [
        "Next we create train and test dataset, then we use tensorflow 2 to post-pad the sequence to max sequence with 0. Our labels are changed to a one-hot vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tJxtPtohHQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36fb9b9b-8961-4cea-9fe6-17d16d87006e"
      },
      "source": [
        "%%time\n",
        "x_train = np.asarray([sent2features(sent, embeddings) for sent in train_data])\n",
        "y_train = [sent2labels(sent) for sent in train_data]\n",
        "x_test = [sent2features(sent, embeddings) for sent in test_data]\n",
        "y_test = [sent2labels(sent) for sent in test_data]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 308 ms, sys: 0 ns, total: 308 ms\n",
            "Wall time: 314 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG1gvJ4mhHQJ"
      },
      "source": [
        "x_train=tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "y_train=tf.keras.preprocessing.sequence.pad_sequences(y_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "x_test=tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=102, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "y_temp =[]\n",
        "for i in range(len(y_train)):\n",
        "    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\n",
        "y_train = np.asarray(y_temp).reshape(-1,102,n_classes)\n",
        "del(y_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU9x6VdehHQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921b1896-d133-497a-f37d-f6ac9bc361fa"
      },
      "source": [
        "print(x_train[100],x_train.shape)\n",
        "print(y_train[100][3],y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 29 327   5 328   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0] (18500, 102)\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (18500, 102, 48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx_c3-LwhHQP"
      },
      "source": [
        "## 3. Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvCW24orhHQP"
      },
      "source": [
        "Our output from tf keras is a distribution of problabilities on all possible label. outputToLabel will return an indices of maximum problability from output sequence.\n",
        "\n",
        "evaluation_report is the same as in the demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqgp2gd4hHQQ"
      },
      "source": [
        "def outputToLabel(yt,seq_len):\n",
        "    out = []\n",
        "    for i in range(0,len(yt)):\n",
        "        if(i==seq_len):\n",
        "            break\n",
        "        out.append(np.argmax(yt[i]))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzIL_rsAhHQT"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "def evaluation_report(y_true, y_pred):\n",
        "    # retrieve all tags in y_true\n",
        "    tag_set = set()\n",
        "    for sent in y_true:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    for sent in y_pred:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    tag_list = sorted(list(tag_set))\n",
        "    \n",
        "    # count correct points\n",
        "    tag_info = dict()\n",
        "    for tag in tag_list:\n",
        "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
        "\n",
        "    all_correct = 0\n",
        "    all_count = sum([len(sent) for sent in y_true])\n",
        "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
        "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
        "            if tag_true == tag_pred:\n",
        "                tag_info[tag_true]['correct_tagged'] += 1\n",
        "                all_correct += 1\n",
        "            tag_info[tag_true]['y_true'] += 1\n",
        "            tag_info[tag_pred]['y_pred'] += 1\n",
        "    accuracy = (all_correct / all_count) * 100\n",
        "            \n",
        "    # summarize and make evaluation result\n",
        "    eval_list = list()\n",
        "    for tag in tag_list:\n",
        "        eval_result = dict()\n",
        "        eval_result['tag'] = tag\n",
        "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
        "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
        "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
        "        eval_result['precision'] = precision\n",
        "        eval_result['recall'] = recall\n",
        "        eval_result['f_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
        "        \n",
        "        eval_list.append(eval_result)\n",
        "\n",
        "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f_score': ''})\n",
        "    \n",
        "    df = pd.DataFrame.from_dict(eval_list)\n",
        "    df = df[['tag', 'precision', 'recall', 'f_score', 'correct_count']]\n",
        "    display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YG-RiHdhHQV"
      },
      "source": [
        "## 4. Train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HrkAiMFhHQW"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U75Ivn2vhHQZ"
      },
      "source": [
        "The model is this section is separated to two groups\n",
        "\n",
        "- Neural POS Tagger (4.1)\n",
        "- Neural CRF POS Tagger (4.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLwL3B7rhHQZ"
      },
      "source": [
        "## 4.1.1 Neural POS Tagger  (Example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVoB-1XVhHQa"
      },
      "source": [
        "We create a simple Neural POS Tagger as an example for you. This model dosen't use any pretrained word embbeding so it need to use Embedding layer to train the word embedding from scratch.\n",
        "\n",
        "Instead of using tensorflow.keras.models.Sequential, we use tensorflow.keras.models.Model. The latter is better as it can have multiple input/output, of which Sequential model could not. Due to this reason, the Model class is widely used for building a complex deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxBvv9qfhHQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b777ce5-522c-42a5-b50b-7d423f1e9cd2"
      },
      "source": [
        "inputs = Input(shape=(102,), dtype='int32')\n",
        "output = (Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))(inputs)\n",
        "output = Bidirectional(GRU(32, return_sequences=True))(output)\n",
        "output = Dropout(0.2)(output)\n",
        "output = TimeDistributed(Dense(n_classes,activation='softmax'))(output)\n",
        "model = Model(inputs, output)\n",
        "model.compile(optimizer=Adam(lr=0.001),  loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "model.summary()\n",
        "model.fit(x_train,y=y_train, batch_size=64,epochs=10,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 102)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 102, 32)           480608    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 102, 64)           12672     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 102, 64)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 102, 48)           3120      \n",
            "=================================================================\n",
            "Total params: 496,400\n",
            "Trainable params: 496,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "290/290 [==============================] - 46s 135ms/step - loss: 0.3918 - categorical_accuracy: 0.3598\n",
            "Epoch 2/10\n",
            "290/290 [==============================] - 36s 125ms/step - loss: 0.0690 - categorical_accuracy: 0.8921\n",
            "Epoch 3/10\n",
            "290/290 [==============================] - 35s 122ms/step - loss: 0.0379 - categorical_accuracy: 0.9331\n",
            "Epoch 4/10\n",
            "290/290 [==============================] - 35s 120ms/step - loss: 0.0290 - categorical_accuracy: 0.9455\n",
            "Epoch 5/10\n",
            "290/290 [==============================] - 35s 120ms/step - loss: 0.0247 - categorical_accuracy: 0.9509\n",
            "Epoch 6/10\n",
            "290/290 [==============================] - 35s 120ms/step - loss: 0.0227 - categorical_accuracy: 0.9547\n",
            "Epoch 7/10\n",
            "290/290 [==============================] - 35s 120ms/step - loss: 0.0213 - categorical_accuracy: 0.9565\n",
            "Epoch 8/10\n",
            "290/290 [==============================] - 35s 121ms/step - loss: 0.0196 - categorical_accuracy: 0.9589\n",
            "Epoch 9/10\n",
            "290/290 [==============================] - 35s 119ms/step - loss: 0.0190 - categorical_accuracy: 0.9598\n",
            "Epoch 10/10\n",
            "290/290 [==============================] - 35s 120ms/step - loss: 0.0182 - categorical_accuracy: 0.9617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fccdb543278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo9Da8MThHQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62a6858-f730-4b97-eaf8-4dee0ebbdb02"
      },
      "source": [
        "%%time\n",
        "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "290/290 [==============================] - 34s 119ms/step - loss: 0.0176 - categorical_accuracy: 0.9624\n",
            "Epoch 2/10\n",
            "290/290 [==============================] - 34s 118ms/step - loss: 0.0169 - categorical_accuracy: 0.9641\n",
            "Epoch 3/10\n",
            "290/290 [==============================] - 34s 118ms/step - loss: 0.0164 - categorical_accuracy: 0.9649\n",
            "Epoch 4/10\n",
            "290/290 [==============================] - 35s 120ms/step - loss: 0.0157 - categorical_accuracy: 0.9663\n",
            "Epoch 5/10\n",
            "290/290 [==============================] - 34s 118ms/step - loss: 0.0152 - categorical_accuracy: 0.9673\n",
            "Epoch 6/10\n",
            "290/290 [==============================] - 34s 119ms/step - loss: 0.0147 - categorical_accuracy: 0.9687\n",
            "Epoch 7/10\n",
            "290/290 [==============================] - 34s 119ms/step - loss: 0.0143 - categorical_accuracy: 0.9691\n",
            "Epoch 8/10\n",
            "290/290 [==============================] - 34s 118ms/step - loss: 0.0138 - categorical_accuracy: 0.9702\n",
            "Epoch 9/10\n",
            "290/290 [==============================] - 35s 119ms/step - loss: 0.0134 - categorical_accuracy: 0.9709\n",
            "Epoch 10/10\n",
            "290/290 [==============================] - 35s 119ms/step - loss: 0.0130 - categorical_accuracy: 0.9716\n",
            "CPU times: user 10min 22s, sys: 13.5 s, total: 10min 35s\n",
            "Wall time: 5min 44s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fccda9d3780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yusa214hhHQh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23055ca1-c2ae-4a6b-ea9b-968c5a5b9f50"
      },
      "source": [
        "%%time\n",
        "#model.save_weights('/data/my_pos_no_crf.h5')\n",
        "#model.load_weights('/data/my_pos_no_crf.h5')\n",
        "y_pred=model.predict(x_test)\n",
        "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
        "evaluation_report(y_test, ypred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>99.8099</td>\n",
              "      <td>99.7558</td>\n",
              "      <td>99.7828</td>\n",
              "      <td>3676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>94.1769</td>\n",
              "      <td>94.1198</td>\n",
              "      <td>94.1483</td>\n",
              "      <td>7763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>91.23</td>\n",
              "      <td>95.7783</td>\n",
              "      <td>93.4489</td>\n",
              "      <td>16176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>99.9845</td>\n",
              "      <td>99.5357</td>\n",
              "      <td>99.7596</td>\n",
              "      <td>12862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>87.8378</td>\n",
              "      <td>97.0149</td>\n",
              "      <td>92.1986</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>99.359</td>\n",
              "      <td>89.0805</td>\n",
              "      <td>93.9394</td>\n",
              "      <td>465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>96.9334</td>\n",
              "      <td>97.3064</td>\n",
              "      <td>97.1195</td>\n",
              "      <td>2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>80</td>\n",
              "      <td>52.0482</td>\n",
              "      <td>63.0657</td>\n",
              "      <td>216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>65.2632</td>\n",
              "      <td>67.3913</td>\n",
              "      <td>66.3102</td>\n",
              "      <td>248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>61.1212</td>\n",
              "      <td>40.2861</td>\n",
              "      <td>48.5632</td>\n",
              "      <td>338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>84.1584</td>\n",
              "      <td>98.8372</td>\n",
              "      <td>90.9091</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>94.8967</td>\n",
              "      <td>97.5031</td>\n",
              "      <td>96.1823</td>\n",
              "      <td>781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>89.9268</td>\n",
              "      <td>85.448</td>\n",
              "      <td>87.6302</td>\n",
              "      <td>3071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>92.9947</td>\n",
              "      <td>95.0446</td>\n",
              "      <td>94.0085</td>\n",
              "      <td>5217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>78.5092</td>\n",
              "      <td>72.2816</td>\n",
              "      <td>75.2668</td>\n",
              "      <td>811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>88.291</td>\n",
              "      <td>85.3872</td>\n",
              "      <td>86.8148</td>\n",
              "      <td>2051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>97.4206</td>\n",
              "      <td>84.0753</td>\n",
              "      <td>90.2574</td>\n",
              "      <td>491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>97.4403</td>\n",
              "      <td>98.8745</td>\n",
              "      <td>98.1521</td>\n",
              "      <td>1142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>97.0845</td>\n",
              "      <td>96.5217</td>\n",
              "      <td>96.8023</td>\n",
              "      <td>333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>97.9452</td>\n",
              "      <td>96.9492</td>\n",
              "      <td>97.4446</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>96.4689</td>\n",
              "      <td>90.165</td>\n",
              "      <td>93.2105</td>\n",
              "      <td>1366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>79.7238</td>\n",
              "      <td>79.2265</td>\n",
              "      <td>79.4743</td>\n",
              "      <td>1270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>90.2174</td>\n",
              "      <td>94.1176</td>\n",
              "      <td>92.1263</td>\n",
              "      <td>1328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>81.9959</td>\n",
              "      <td>87.1038</td>\n",
              "      <td>84.4727</td>\n",
              "      <td>797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>81.3609</td>\n",
              "      <td>66.586</td>\n",
              "      <td>73.2357</td>\n",
              "      <td>275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>93.7853</td>\n",
              "      <td>94.3182</td>\n",
              "      <td>94.051</td>\n",
              "      <td>166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>92.7928</td>\n",
              "      <td>78.626</td>\n",
              "      <td>85.124</td>\n",
              "      <td>103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>29</td>\n",
              "      <td>93.8462</td>\n",
              "      <td>96.519</td>\n",
              "      <td>95.1638</td>\n",
              "      <td>305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>30</td>\n",
              "      <td>71.9626</td>\n",
              "      <td>75.4902</td>\n",
              "      <td>73.6842</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>31</td>\n",
              "      <td>46.7456</td>\n",
              "      <td>76.699</td>\n",
              "      <td>58.0882</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>32</td>\n",
              "      <td>63.2184</td>\n",
              "      <td>61.7978</td>\n",
              "      <td>62.5</td>\n",
              "      <td>110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>33</td>\n",
              "      <td>89.4737</td>\n",
              "      <td>50</td>\n",
              "      <td>64.1509</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>34</td>\n",
              "      <td>91.6201</td>\n",
              "      <td>87.5445</td>\n",
              "      <td>89.5359</td>\n",
              "      <td>492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>35</td>\n",
              "      <td>33.3333</td>\n",
              "      <td>55.5556</td>\n",
              "      <td>41.6667</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>36</td>\n",
              "      <td>100</td>\n",
              "      <td>93.75</td>\n",
              "      <td>96.7742</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>37</td>\n",
              "      <td>89.3805</td>\n",
              "      <td>99.0196</td>\n",
              "      <td>93.9535</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>38</td>\n",
              "      <td>52.7778</td>\n",
              "      <td>48.7179</td>\n",
              "      <td>50.6667</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>39</td>\n",
              "      <td>73.9437</td>\n",
              "      <td>75</td>\n",
              "      <td>74.4681</td>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>40</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>41</td>\n",
              "      <td>80</td>\n",
              "      <td>80</td>\n",
              "      <td>80</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>42</td>\n",
              "      <td>100</td>\n",
              "      <td>88.2353</td>\n",
              "      <td>93.75</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>43</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>accuracy=92.89</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               tag precision   recall  f_score correct_count\n",
              "0                1   99.8099  99.7558  99.7828          3676\n",
              "1                2   94.1769  94.1198  94.1483          7763\n",
              "2                3     91.23  95.7783  93.4489         16176\n",
              "3                4   99.9845  99.5357  99.7596         12862\n",
              "4                5   87.8378  97.0149  92.1986            65\n",
              "5                6    99.359  89.0805  93.9394           465\n",
              "6                7   96.9334  97.3064  97.1195          2023\n",
              "7                8        80  52.0482  63.0657           216\n",
              "8                9   65.2632  67.3913  66.3102           248\n",
              "9               10   61.1212  40.2861  48.5632           338\n",
              "10              11   84.1584  98.8372  90.9091            85\n",
              "11              12   94.8967  97.5031  96.1823           781\n",
              "12              13   89.9268   85.448  87.6302          3071\n",
              "13              14   92.9947  95.0446  94.0085          5217\n",
              "14              15   78.5092  72.2816  75.2668           811\n",
              "15              16    88.291  85.3872  86.8148          2051\n",
              "16              17   97.4206  84.0753  90.2574           491\n",
              "17              18   97.4403  98.8745  98.1521          1142\n",
              "18              19   97.0845  96.5217  96.8023           333\n",
              "19              20   97.9452  96.9492  97.4446           286\n",
              "20              21   96.4689   90.165  93.2105          1366\n",
              "21              22   79.7238  79.2265  79.4743          1270\n",
              "22              23   90.2174  94.1176  92.1263          1328\n",
              "23              24   81.9959  87.1038  84.4727           797\n",
              "24              25   81.3609   66.586  73.2357           275\n",
              "25              26   93.7853  94.3182   94.051           166\n",
              "26              27   92.7928   78.626   85.124           103\n",
              "27              29   93.8462   96.519  95.1638           305\n",
              "28              30   71.9626  75.4902  73.6842            77\n",
              "29              31   46.7456   76.699  58.0882            79\n",
              "30              32   63.2184  61.7978     62.5           110\n",
              "31              33   89.4737       50  64.1509            34\n",
              "32              34   91.6201  87.5445  89.5359           492\n",
              "33              35   33.3333  55.5556  41.6667             5\n",
              "34              36       100    93.75  96.7742            15\n",
              "35              37   89.3805  99.0196  93.9535           101\n",
              "36              38   52.7778  48.7179  50.6667            19\n",
              "37              39   73.9437       75  74.4681           105\n",
              "38              40       100      100      100           280\n",
              "39              41        80       80       80            16\n",
              "40              42       100  88.2353    93.75            15\n",
              "41              43         -        0        -             0\n",
              "42              45         0        0        -             0\n",
              "43              46         0        0        -             0\n",
              "44  accuracy=92.89                                          "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7.32 s, sys: 247 ms, total: 7.57 s\n",
            "Wall time: 5.36 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0NKka14hHQw"
      },
      "source": [
        "## 4.2 CRF Viterbi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijd1rwTghHQx"
      },
      "source": [
        "Your next task is to incorporate Conditional random fields (CRF) to your model.\n",
        "\n",
        "To use the CRF layer, you need to use an extension repository for tensorflow library, call tf2crf. If you want to see the detailed implementation, you should read the official tensorflow extention of CRF (https://www.tensorflow.org/addons/api_docs/python/tfa/text).\n",
        "\n",
        "tf2crf link :  https://github.com/xuxingya/tf2crf\n",
        "\n",
        "For inference, you should look at crf.py at the method call and view the input/output argmunets. \n",
        "Link : https://github.com/xuxingya/tf2crf/blob/master/tf2crf/crf.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuybajePhHQy"
      },
      "source": [
        "### 4.2.1 CRF without pretrained weight\n",
        "### #TODO 1\n",
        "Incoperate CRF layer to your model in 4.1. CRF is quite complex compare to previous example model, so you should train it with more epoch, so it can converge.\n",
        "\n",
        "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
        "\n",
        "Do not forget to save this model weight."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEV0q1vAhHQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c80e0f-dff5-4890-a7b5-8c16a07aa108"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tf2crf import CRF, ModelWithCRFLoss\n",
        "# INSERT YOUR CODE HERE\n",
        "inputs = Input(shape=(102,), dtype='int32')\n",
        "output = (Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))(inputs)\n",
        "output = Bidirectional(GRU(32, return_sequences=True))(output)\n",
        "output = Dropout(0.2)(output)\n",
        "#output = Dense(n_classes, activation=None)(output)\n",
        "output = TimeDistributed(Dense(n_classes,activation=None))(output)\n",
        "crf = CRF(dtype='float32')\n",
        "output = crf(output)\n",
        "base_model = Model(inputs, output)\n",
        "model = ModelWithCRFLoss(base_model)\n",
        "#model.compile(optimizer=Adam(lr=0.001),  loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.compile(optimizer='adam')\n",
        "\n",
        "y_crf_train = [[np.argmax(word) for word in sentence ] for sentence in y_train]\n",
        "y_crf_train = tf.keras.preprocessing.sequence.pad_sequences(y_crf_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "\n",
        "#model.summary()\n",
        "model.fit(x_train,y=y_crf_train, batch_size=64,epochs=10,verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "290/290 [==============================] - 92s 287ms/step - crf_loss: 26.6608 - accuracy: 0.5541\n",
            "Epoch 2/10\n",
            "290/290 [==============================] - 84s 291ms/step - crf_loss: 5.8926 - accuracy: 0.9063\n",
            "Epoch 3/10\n",
            "290/290 [==============================] - 84s 290ms/step - crf_loss: 3.5733 - accuracy: 0.9368\n",
            "Epoch 4/10\n",
            "290/290 [==============================] - 84s 289ms/step - crf_loss: 2.8150 - accuracy: 0.9470\n",
            "Epoch 5/10\n",
            "290/290 [==============================] - 84s 291ms/step - crf_loss: 2.4819 - accuracy: 0.9518\n",
            "Epoch 6/10\n",
            "290/290 [==============================] - 84s 291ms/step - crf_loss: 2.2779 - accuracy: 0.9545\n",
            "Epoch 7/10\n",
            "290/290 [==============================] - 84s 291ms/step - crf_loss: 2.1243 - accuracy: 0.9568\n",
            "Epoch 8/10\n",
            "290/290 [==============================] - 84s 291ms/step - crf_loss: 2.0133 - accuracy: 0.9585\n",
            "Epoch 9/10\n",
            "290/290 [==============================] - 85s 291ms/step - crf_loss: 1.9214 - accuracy: 0.9602\n",
            "Epoch 10/10\n",
            "290/290 [==============================] - 84s 289ms/step - crf_loss: 1.8311 - accuracy: 0.9616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fccd9e26198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lqBMZN1t2EkC",
        "outputId": "aa801b93-ad2b-44e6-af6a-f5a10b1aaf15"
      },
      "source": [
        "#model.save_weights('/data/my_pos_no_crf.h5')\n",
        "#model.load_weights('/data/my_pos_no_crf.h5')\n",
        "y_pred=model.predict(x_test)\n",
        "\n",
        "evaluation_report(y_test, y_pred[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>99.864</td>\n",
              "      <td>99.6201</td>\n",
              "      <td>99.7419</td>\n",
              "      <td>3671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>95.3181</td>\n",
              "      <td>93.55</td>\n",
              "      <td>94.4257</td>\n",
              "      <td>7716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>91.1172</td>\n",
              "      <td>95.2336</td>\n",
              "      <td>93.1299</td>\n",
              "      <td>16084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>99.8759</td>\n",
              "      <td>99.6363</td>\n",
              "      <td>99.7559</td>\n",
              "      <td>12875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>95.6522</td>\n",
              "      <td>98.5075</td>\n",
              "      <td>97.0588</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>99.7802</td>\n",
              "      <td>86.9732</td>\n",
              "      <td>92.9376</td>\n",
              "      <td>454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>98.0985</td>\n",
              "      <td>96.7773</td>\n",
              "      <td>97.4334</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>70.1863</td>\n",
              "      <td>54.4578</td>\n",
              "      <td>61.3297</td>\n",
              "      <td>226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>76.1246</td>\n",
              "      <td>59.7826</td>\n",
              "      <td>66.9711</td>\n",
              "      <td>220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>62.6984</td>\n",
              "      <td>47.0799</td>\n",
              "      <td>53.7781</td>\n",
              "      <td>395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>84.3137</td>\n",
              "      <td>100</td>\n",
              "      <td>91.4894</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>95.679</td>\n",
              "      <td>96.7541</td>\n",
              "      <td>96.2135</td>\n",
              "      <td>775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>88.8921</td>\n",
              "      <td>86.1714</td>\n",
              "      <td>87.5106</td>\n",
              "      <td>3097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>91.9923</td>\n",
              "      <td>96.0649</td>\n",
              "      <td>93.9845</td>\n",
              "      <td>5273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>81.59</td>\n",
              "      <td>69.5187</td>\n",
              "      <td>75.0722</td>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>89.7221</td>\n",
              "      <td>84.6794</td>\n",
              "      <td>87.1279</td>\n",
              "      <td>2034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>97.6608</td>\n",
              "      <td>85.7877</td>\n",
              "      <td>91.34</td>\n",
              "      <td>501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>96.8724</td>\n",
              "      <td>99.2208</td>\n",
              "      <td>98.0325</td>\n",
              "      <td>1146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>97.9412</td>\n",
              "      <td>96.5217</td>\n",
              "      <td>97.2263</td>\n",
              "      <td>333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>98.2818</td>\n",
              "      <td>96.9492</td>\n",
              "      <td>97.6109</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>96.0219</td>\n",
              "      <td>92.4092</td>\n",
              "      <td>94.181</td>\n",
              "      <td>1400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>78.3622</td>\n",
              "      <td>81.7842</td>\n",
              "      <td>80.0366</td>\n",
              "      <td>1311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>88.8449</td>\n",
              "      <td>95.3933</td>\n",
              "      <td>92.0027</td>\n",
              "      <td>1346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>85.0109</td>\n",
              "      <td>84.918</td>\n",
              "      <td>84.9645</td>\n",
              "      <td>777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>84.2857</td>\n",
              "      <td>85.7143</td>\n",
              "      <td>84.994</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>96.2733</td>\n",
              "      <td>88.0682</td>\n",
              "      <td>91.9881</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>92.1739</td>\n",
              "      <td>80.916</td>\n",
              "      <td>86.1789</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>92.2156</td>\n",
              "      <td>97.4684</td>\n",
              "      <td>94.7692</td>\n",
              "      <td>308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>72.1154</td>\n",
              "      <td>73.5294</td>\n",
              "      <td>72.8155</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>52.0958</td>\n",
              "      <td>84.466</td>\n",
              "      <td>64.4444</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>65.7895</td>\n",
              "      <td>56.1798</td>\n",
              "      <td>60.6061</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>77.7778</td>\n",
              "      <td>51.4706</td>\n",
              "      <td>61.9469</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>90.9263</td>\n",
              "      <td>85.5872</td>\n",
              "      <td>88.176</td>\n",
              "      <td>481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>83.3333</td>\n",
              "      <td>55.5556</td>\n",
              "      <td>66.6667</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>100</td>\n",
              "      <td>75</td>\n",
              "      <td>85.7143</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>37</td>\n",
              "      <td>92.6606</td>\n",
              "      <td>99.0196</td>\n",
              "      <td>95.7346</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>67.7419</td>\n",
              "      <td>53.8462</td>\n",
              "      <td>60</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>39</td>\n",
              "      <td>75.625</td>\n",
              "      <td>86.4286</td>\n",
              "      <td>80.6667</td>\n",
              "      <td>121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>41</td>\n",
              "      <td>80</td>\n",
              "      <td>80</td>\n",
              "      <td>80</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>42</td>\n",
              "      <td>100</td>\n",
              "      <td>88.2353</td>\n",
              "      <td>93.75</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>30</td>\n",
              "      <td>33.3333</td>\n",
              "      <td>31.5789</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>45</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>46</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>accuracy=93.01</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               tag precision   recall  f_score correct_count\n",
              "0                0         -        0        -             0\n",
              "1                1    99.864  99.6201  99.7419          3671\n",
              "2                2   95.3181    93.55  94.4257          7716\n",
              "3                3   91.1172  95.2336  93.1299         16084\n",
              "4                4   99.8759  99.6363  99.7559         12875\n",
              "5                5   95.6522  98.5075  97.0588            66\n",
              "6                6   99.7802  86.9732  92.9376           454\n",
              "7                7   98.0985  96.7773  97.4334          2012\n",
              "8                8   70.1863  54.4578  61.3297           226\n",
              "9                9   76.1246  59.7826  66.9711           220\n",
              "10              10   62.6984  47.0799  53.7781           395\n",
              "11              11   84.3137      100  91.4894            86\n",
              "12              12    95.679  96.7541  96.2135           775\n",
              "13              13   88.8921  86.1714  87.5106          3097\n",
              "14              14   91.9923  96.0649  93.9845          5273\n",
              "15              15     81.59  69.5187  75.0722           780\n",
              "16              16   89.7221  84.6794  87.1279          2034\n",
              "17              17   97.6608  85.7877    91.34           501\n",
              "18              18   96.8724  99.2208  98.0325          1146\n",
              "19              19   97.9412  96.5217  97.2263           333\n",
              "20              20   98.2818  96.9492  97.6109           286\n",
              "21              21   96.0219  92.4092   94.181          1400\n",
              "22              22   78.3622  81.7842  80.0366          1311\n",
              "23              23   88.8449  95.3933  92.0027          1346\n",
              "24              24   85.0109   84.918  84.9645           777\n",
              "25              25   84.2857  85.7143   84.994           354\n",
              "26              26   96.2733  88.0682  91.9881           155\n",
              "27              27   92.1739   80.916  86.1789           106\n",
              "28              29   92.2156  97.4684  94.7692           308\n",
              "29              30   72.1154  73.5294  72.8155            75\n",
              "30              31   52.0958   84.466  64.4444            87\n",
              "31              32   65.7895  56.1798  60.6061           100\n",
              "32              33   77.7778  51.4706  61.9469            35\n",
              "33              34   90.9263  85.5872   88.176           481\n",
              "34              35   83.3333  55.5556  66.6667             5\n",
              "35              36       100       75  85.7143            12\n",
              "36              37   92.6606  99.0196  95.7346           101\n",
              "37              38   67.7419  53.8462       60            21\n",
              "38              39    75.625  86.4286  80.6667           121\n",
              "39              40       100      100      100           280\n",
              "40              41        80       80       80            16\n",
              "41              42       100  88.2353    93.75            15\n",
              "42              43        30  33.3333  31.5789             3\n",
              "43              45         -        0        -             0\n",
              "44              46         -        0        -             0\n",
              "45  accuracy=93.01                                          "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOnHO-__kj7T"
      },
      "source": [
        "\n",
        "### 4.2.2 CRF with pretrained weight\n",
        "\n",
        "### #TODO 2\n",
        "\n",
        "We would like you create a neural CRF POS tagger model  with the pretrained word embedding as an input and the word embedding is trainable (not fixed). To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
        "\n",
        "Please note that the given pretrained word embedding only have weights for the vocabuary in BEST corpus.\n",
        "\n",
        "Optionally, you can use your own pretrained word embedding.\n",
        "\n",
        "#### Hint: You can get the embedding from get_embeddings function from embeddings/emb_reader.py . \n",
        "\n",
        "(You may want to read about Tensorflow Masking layer and Trainable parameter)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKAeLKA5oAeP"
      },
      "source": [
        "# INSERT YOUR CODE HERE\n",
        "from resources.embeddings.emb_reader import get_embeddings\n",
        "embeddings_index = get_embeddings()\n",
        "embedding_matrix = np.zeros((len(word_to_idx) , 64))\n",
        "\n",
        "for word, i in word_to_idx.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVBAv3a9kanH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c1e823e-24b3-447e-94aa-da71a77cc33c"
      },
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tf2crf import CRF, ModelWithCRFLoss\n",
        "\n",
        "inputs = Input(shape=(102,), dtype='int32')\n",
        "#output = (Embedding(len(word_to_idx),64,weights=[embedding_matrix],input_length=102,trainable=False,mask_zero=True))(inputs)\n",
        "output = (Embedding(len(word_to_idx),64,embeddings_initializer=Constant(embedding_matrix),input_length=102,trainable=True,mask_zero=True))(inputs)\n",
        "output = Bidirectional(GRU(64, return_sequences=True))(output)\n",
        "output = Dropout(0.2)(output)\n",
        "output = TimeDistributed(Dense(n_classes,activation=None))(output)\n",
        "crf = CRF(dtype='float32')\n",
        "output = crf(output)\n",
        "base_model = Model(inputs, output)\n",
        "model = ModelWithCRFLoss(base_model)\n",
        "model.compile(optimizer='adam')\n",
        "\n",
        "y_crf_train = [[np.argmax(word) for word in sentence ] for sentence in y_train]\n",
        "y_crf_train = tf.keras.preprocessing.sequence.pad_sequences(y_crf_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "\n",
        "#model.summary()\n",
        "model.fit(x_train,y=y_crf_train, batch_size=64,epochs=10,verbose=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "290/290 [==============================] - 113s 356ms/step - crf_loss: 18.3623 - accuracy: 0.6909\n",
            "Epoch 2/10\n",
            "290/290 [==============================] - 104s 360ms/step - crf_loss: 3.9581 - accuracy: 0.9291\n",
            "Epoch 3/10\n",
            "290/290 [==============================] - 104s 357ms/step - crf_loss: 2.7292 - accuracy: 0.9481\n",
            "Epoch 4/10\n",
            "290/290 [==============================] - 104s 360ms/step - crf_loss: 2.2935 - accuracy: 0.9548\n",
            "Epoch 5/10\n",
            "290/290 [==============================] - 105s 362ms/step - crf_loss: 2.0575 - accuracy: 0.9580\n",
            "Epoch 6/10\n",
            "290/290 [==============================] - 104s 359ms/step - crf_loss: 1.9137 - accuracy: 0.9602\n",
            "Epoch 7/10\n",
            "290/290 [==============================] - 104s 358ms/step - crf_loss: 1.7895 - accuracy: 0.9622\n",
            "Epoch 8/10\n",
            "290/290 [==============================] - 104s 357ms/step - crf_loss: 1.6840 - accuracy: 0.9645\n",
            "Epoch 9/10\n",
            "290/290 [==============================] - 104s 359ms/step - crf_loss: 1.5892 - accuracy: 0.9659\n",
            "Epoch 10/10\n",
            "290/290 [==============================] - 104s 358ms/step - crf_loss: 1.5077 - accuracy: 0.9676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fccd4bd2c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c2MErX0ByA26",
        "outputId": "0f3a8037-4ba4-4710-d152-1ed3839a1ff7"
      },
      "source": [
        "y_pred=model.predict(x_test)\n",
        "evaluation_report(y_test, y_pred[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>99.7828</td>\n",
              "      <td>99.7286</td>\n",
              "      <td>99.7557</td>\n",
              "      <td>3675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>94.2497</td>\n",
              "      <td>95.1867</td>\n",
              "      <td>94.7159</td>\n",
              "      <td>7851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>91.287</td>\n",
              "      <td>96.3408</td>\n",
              "      <td>93.7459</td>\n",
              "      <td>16271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>99.9844</td>\n",
              "      <td>99.3886</td>\n",
              "      <td>99.6856</td>\n",
              "      <td>12843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>82.5</td>\n",
              "      <td>98.5075</td>\n",
              "      <td>89.7959</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>99.5807</td>\n",
              "      <td>90.9962</td>\n",
              "      <td>95.0951</td>\n",
              "      <td>475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>98.1086</td>\n",
              "      <td>97.3064</td>\n",
              "      <td>97.7059</td>\n",
              "      <td>2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>67.4359</td>\n",
              "      <td>63.3735</td>\n",
              "      <td>65.3416</td>\n",
              "      <td>263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>58.8235</td>\n",
              "      <td>62.5</td>\n",
              "      <td>60.6061</td>\n",
              "      <td>230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>63.6697</td>\n",
              "      <td>41.3588</td>\n",
              "      <td>50.1445</td>\n",
              "      <td>347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>88.172</td>\n",
              "      <td>95.3488</td>\n",
              "      <td>91.6201</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>94.964</td>\n",
              "      <td>98.8764</td>\n",
              "      <td>96.8807</td>\n",
              "      <td>792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>89.7776</td>\n",
              "      <td>86.5053</td>\n",
              "      <td>88.1111</td>\n",
              "      <td>3109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>94.9752</td>\n",
              "      <td>94.3523</td>\n",
              "      <td>94.6628</td>\n",
              "      <td>5179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>82.5558</td>\n",
              "      <td>72.549</td>\n",
              "      <td>77.2296</td>\n",
              "      <td>814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>88.4263</td>\n",
              "      <td>88.4263</td>\n",
              "      <td>88.4263</td>\n",
              "      <td>2124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>97.9127</td>\n",
              "      <td>88.3562</td>\n",
              "      <td>92.8893</td>\n",
              "      <td>516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>98.5395</td>\n",
              "      <td>99.3074</td>\n",
              "      <td>98.9219</td>\n",
              "      <td>1147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>97.3684</td>\n",
              "      <td>96.5217</td>\n",
              "      <td>96.9432</td>\n",
              "      <td>333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>98.9619</td>\n",
              "      <td>96.9492</td>\n",
              "      <td>97.9452</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>94.5674</td>\n",
              "      <td>93.0693</td>\n",
              "      <td>93.8124</td>\n",
              "      <td>1410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>85.1414</td>\n",
              "      <td>80.786</td>\n",
              "      <td>82.9065</td>\n",
              "      <td>1295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>89.8192</td>\n",
              "      <td>95.039</td>\n",
              "      <td>92.3554</td>\n",
              "      <td>1341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>94.3949</td>\n",
              "      <td>80.9836</td>\n",
              "      <td>87.1765</td>\n",
              "      <td>741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>93.0921</td>\n",
              "      <td>68.523</td>\n",
              "      <td>78.94</td>\n",
              "      <td>283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>95.3488</td>\n",
              "      <td>93.1818</td>\n",
              "      <td>94.2529</td>\n",
              "      <td>164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>92.3077</td>\n",
              "      <td>82.4427</td>\n",
              "      <td>87.0968</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>93.7107</td>\n",
              "      <td>94.3038</td>\n",
              "      <td>94.0063</td>\n",
              "      <td>298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>71.2963</td>\n",
              "      <td>75.4902</td>\n",
              "      <td>73.3333</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>56.8627</td>\n",
              "      <td>84.466</td>\n",
              "      <td>67.9687</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>69.3182</td>\n",
              "      <td>68.5393</td>\n",
              "      <td>68.9266</td>\n",
              "      <td>122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>91.6667</td>\n",
              "      <td>48.5294</td>\n",
              "      <td>63.4615</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>95.0998</td>\n",
              "      <td>93.2384</td>\n",
              "      <td>94.1599</td>\n",
              "      <td>524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>83.3333</td>\n",
              "      <td>55.5556</td>\n",
              "      <td>66.6667</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>100</td>\n",
              "      <td>93.75</td>\n",
              "      <td>96.7742</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>37</td>\n",
              "      <td>92.6606</td>\n",
              "      <td>99.0196</td>\n",
              "      <td>95.7346</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>60.6061</td>\n",
              "      <td>51.2821</td>\n",
              "      <td>55.5556</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>39</td>\n",
              "      <td>80.3922</td>\n",
              "      <td>87.8571</td>\n",
              "      <td>83.959</td>\n",
              "      <td>123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>41</td>\n",
              "      <td>88.8889</td>\n",
              "      <td>80</td>\n",
              "      <td>84.2105</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>42</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>45</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>46</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>accuracy=93.50</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               tag precision   recall  f_score correct_count\n",
              "0                0         -        0        -             0\n",
              "1                1   99.7828  99.7286  99.7557          3675\n",
              "2                2   94.2497  95.1867  94.7159          7851\n",
              "3                3    91.287  96.3408  93.7459         16271\n",
              "4                4   99.9844  99.3886  99.6856         12843\n",
              "5                5      82.5  98.5075  89.7959            66\n",
              "6                6   99.5807  90.9962  95.0951           475\n",
              "7                7   98.1086  97.3064  97.7059          2023\n",
              "8                8   67.4359  63.3735  65.3416           263\n",
              "9                9   58.8235     62.5  60.6061           230\n",
              "10              10   63.6697  41.3588  50.1445           347\n",
              "11              11    88.172  95.3488  91.6201            82\n",
              "12              12    94.964  98.8764  96.8807           792\n",
              "13              13   89.7776  86.5053  88.1111          3109\n",
              "14              14   94.9752  94.3523  94.6628          5179\n",
              "15              15   82.5558   72.549  77.2296           814\n",
              "16              16   88.4263  88.4263  88.4263          2124\n",
              "17              17   97.9127  88.3562  92.8893           516\n",
              "18              18   98.5395  99.3074  98.9219          1147\n",
              "19              19   97.3684  96.5217  96.9432           333\n",
              "20              20   98.9619  96.9492  97.9452           286\n",
              "21              21   94.5674  93.0693  93.8124          1410\n",
              "22              22   85.1414   80.786  82.9065          1295\n",
              "23              23   89.8192   95.039  92.3554          1341\n",
              "24              24   94.3949  80.9836  87.1765           741\n",
              "25              25   93.0921   68.523    78.94           283\n",
              "26              26   95.3488  93.1818  94.2529           164\n",
              "27              27   92.3077  82.4427  87.0968           108\n",
              "28              29   93.7107  94.3038  94.0063           298\n",
              "29              30   71.2963  75.4902  73.3333            77\n",
              "30              31   56.8627   84.466  67.9687            87\n",
              "31              32   69.3182  68.5393  68.9266           122\n",
              "32              33   91.6667  48.5294  63.4615            33\n",
              "33              34   95.0998  93.2384  94.1599           524\n",
              "34              35   83.3333  55.5556  66.6667             5\n",
              "35              36       100    93.75  96.7742            15\n",
              "36              37   92.6606  99.0196  95.7346           101\n",
              "37              38   60.6061  51.2821  55.5556            20\n",
              "38              39   80.3922  87.8571   83.959           123\n",
              "39              40       100      100      100           280\n",
              "40              41   88.8889       80  84.2105            16\n",
              "41              42       100      100      100            17\n",
              "42              43         0        0        -             0\n",
              "43              45         -        0        -             0\n",
              "44              46         -        0        -             0\n",
              "45  accuracy=93.50                                          "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYX749sbkzA-"
      },
      "source": [
        "### #TODO 3\n",
        "Compare the result between all neural tagger models in 4.1 and 4.2.x and provide a convincing reason and example for the result of these models (which model perform better, why?)\n",
        "\n",
        "(If you use your own weight please state so in the answer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YluVI-trXYN0",
        "outputId": "a40bfb7e-d001-4334-e12d-67bab85d5b50"
      },
      "source": [
        "print(\"จาก model ทั้งสามที่ได้ทำการ train และ predict ผล ได้ข้อสรุปว่า model ทั้งสามได้ accuracy ที่ไม่แตกต่างกันอย่างมีนัยสำคัญ\")\n",
        "print(\"โดย model ที่ดีที่สุดจะเป็น model ที่ได้ใส่ pre-train weight ลงไป อันเนื่องมาจาก pre-train weight ดังกล่าวนั้นได้รับการ train เพื่อให้ได้ค่าที่ถูกต้องมาก่อนแล้ว\")\n",
        "print(\"จึ่งทำให้ model ดังกล่าวได้ accuracy ที่ดีที่สุด(แต่กระนั้นก็ยังไม่ทำให้เกิดความแตกต่างแบบมีนัยสำคัญต่อ model อื่น)\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "จาก model ทั้งสามที่ได้ทำการ train และ predict ผล ได้ข้อสรุปว่า model ทั้งสามได้ accuracy ที่ไม่แตกต่างกันอย่างมีนัยสำคัญ\n",
            "โดย model ที่ดีที่สุดจะเป็น model ที่ได้ใส่ pre-train weight ลงไป อันเนื่องมาจาก pre-train weight ดังกล่าวนั้นได้รับการ train เพื่อให้ได้ค่าที่ถูกต้องมาก่อนแล้ว\n",
            "จึ่งทำให้ model ดังกล่าวได้ accuracy ที่ดีที่สุด(แต่กระนั้นก็ยังไม่ทำให้เกิดความแตกต่างแบบมีนัยสำคัญต่อ model อื่น)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJKtOsoRhHQv"
      },
      "source": [
        "<b>Write your answer here :</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z8raWOawjxp"
      },
      "source": [
        "### #TODO 4\n",
        "\n",
        "Upon inference, the model also returns its transition matrix, which is learned during training. Your task is to observe and report whether the returned matrix is sensible. You can provide some examples to support your argument.\n",
        "\n",
        "#### Hint : The transition matrix must have the shape  of (num_class, num_class).\n",
        "\n",
        "<b>Write your answer here :</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P9_4PrL4J8v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "outputId": "6e4ba62a-2fb8-40bc-e96a-7e717c1434c6"
      },
      "source": [
        "import plotly.express as px\n",
        "transition_matrix = np.array(crf.transitions)\n",
        "fig = px.imshow(transition_matrix )\n",
        "fig.update_xaxes(dtick=1)\n",
        "fig.update_yaxes(dtick=1)\n",
        "fig.update_layout(\n",
        "    width=750,\n",
        "    height=750\n",
        ")\n",
        "fig.show()\n",
        "print(label_to_idx)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"8caa76cc-44e2-4b93-b2a8-62a6b446bde1\" class=\"plotly-graph-div\" style=\"height:750px; width:750px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"8caa76cc-44e2-4b93-b2a8-62a6b446bde1\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '8caa76cc-44e2-4b93-b2a8-62a6b446bde1',\n",
              "                        [{\"coloraxis\": \"coloraxis\", \"type\": \"heatmap\", \"z\": [[-0.05784427747130394, 0.14797009527683258, -0.07490450143814087, 0.021877752617001534, -0.3812068700790405, -0.162551611661911, 0.114313505589962, -0.19365186989307404, 0.08331655710935593, -0.10565150529146194, -0.09955353289842606, 0.21672381460666656, 0.13861779868602753, -0.22832326591014862, -0.05800773575901985, -0.1895369291305542, -0.28428974747657776, -0.07206366211175919, -0.2809010148048401, 0.0814472958445549, -0.07417619973421097, 0.08178947865962982, -0.07901949435472488, -0.011740156449377537, -0.07383186370134354, -0.1731264889240265, -0.1667407751083374, -0.1627882570028305, -0.3467060923576355, 0.012057728134095669, -0.05625836178660393, -0.07525555789470673, 0.030249696224927902, -0.04041597619652748, 0.028168383985757828, -0.19613945484161377, -0.2737314999103546, -0.2555887699127197, -0.14572979509830475, 0.1226322203874588, 0.15179039537906647, 0.13130658864974976, -0.18544714152812958, -0.26203057169914246, -0.03107801266014576, -0.04126200079917908, -0.21509163081645966, 0.2506771683692932], [0.015816720202565193, -0.15247732400894165, -0.009629763662815094, -0.35514602065086365, -0.34077441692352295, -0.016519734635949135, 0.10036817193031311, -0.06214071810245514, -0.029551586136221886, -0.29678967595100403, -0.2108801156282425, -0.41829171776771545, -0.22323215007781982, 0.0461091473698616, -0.4180576801300049, -0.7648999094963074, -0.3022940158843994, -0.4050751030445099, -0.13818494975566864, -0.5731087327003479, -0.228587806224823, -0.34930935502052307, 0.4401184320449829, -0.41417792439460754, -0.260110080242157, -0.06243022903800011, 0.42655542492866516, -0.10221473127603531, -0.06546732783317566, -0.28133487701416016, -0.3612682521343231, -0.17928026616573334, -0.21179074048995972, -0.08822014182806015, -0.37742480635643005, -0.5401555895805359, 0.12147629261016846, -0.299623042345047, -0.20981450378894806, -0.10599929839372635, -0.42966821789741516, 0.02688644453883171, -0.3244016468524933, -0.19814831018447876, -0.20649613440036774, -0.29351821541786194, -0.017021551728248596, 0.04828105494379997], [-0.3018818199634552, -0.03806060180068016, -0.31140050292015076, 0.1370587795972824, -0.22713837027549744, 0.1417783498764038, -0.13530750572681427, 0.03533509746193886, -0.1502409130334854, -0.3047196567058563, 0.016324525699019432, -0.16087007522583008, -0.5595057606697083, -0.2994561493396759, 0.1492643654346466, 0.4673798084259033, 0.3139626979827881, -0.12644632160663605, -0.24431613087654114, -0.04984709993004799, -0.10394232720136642, -0.019405169412493706, -0.5437505841255188, 0.1374228149652481, -0.4158129096031189, -0.4956500828266144, 0.04927100986242294, 0.16722112894058228, -0.16272340714931488, -0.019581642001867294, -0.043524421751499176, -0.04443912208080292, -0.1018773540854454, 0.11469873785972595, -0.28322428464889526, 0.44636163115501404, -0.24746744334697723, -0.5937283039093018, 0.12839485704898834, -0.23486453294754028, -0.11738934367895126, 0.07121514528989792, -0.4206147789955139, -0.3562454283237457, -0.05908368527889252, -0.3610198497772217, 0.19268544018268585, -0.15000559389591217], [-0.17513687908649445, -0.03696318715810776, 0.1428469568490982, 0.1000702977180481, 0.008926290087401867, -0.03902449086308479, 0.37177398800849915, 0.1924910694360733, -0.10178113728761673, 0.11003929376602173, -0.09405067563056946, -0.5401237607002258, 0.08025210350751877, 0.08441991358995438, 0.11702913790941238, -0.2196059674024582, 0.002909578150138259, 0.2635301351547241, 0.014183543622493744, 0.06787377595901489, 0.18196721374988556, 0.1729360967874527, 0.5128792524337769, 0.07387050986289978, -0.47831064462661743, -0.4342702329158783, -0.030674422159790993, -0.2936854362487793, -0.141804501414299, 0.1924542486667633, -0.6104628443717957, 0.33670774102211, 0.2502269446849823, 0.014816522598266602, -0.40406617522239685, 0.08617069572210312, -0.23276765644550323, -0.41839897632598877, -0.07751479744911194, -0.05702584236860275, 0.018184345215559006, 0.19448886811733246, 0.08735312521457672, 0.007680817041546106, 0.01651628501713276, 0.000981200602836907, 0.04225492477416992, -0.38747602701187134], [-0.4258970022201538, 0.057630643248558044, 0.013645674102008343, 0.22765617072582245, 0.12536655366420746, 0.32870978116989136, 0.021878421306610107, 0.17740875482559204, 0.18633346259593964, 0.004455956164747477, 0.17579826712608337, 0.13384945690631866, -0.042185816913843155, -0.07005447149276733, 0.057433657348155975, -0.10940293222665787, 0.06290031969547272, 0.020328780636191368, -0.13821157813072205, -0.09147635102272034, -0.04779089242219925, 0.1890556812286377, -0.3108895719051361, -0.08424244076013565, 0.39140626788139343, 0.1355135142803192, -0.30061355233192444, -0.03156694397330284, -0.1637556254863739, -0.4237399101257324, 0.020986387506127357, 0.04416514188051224, 0.09161460399627686, -0.20467476546764374, 0.009305201470851898, -0.412422239780426, -0.41765570640563965, -0.39289507269859314, -0.06945689022541046, -0.2363147735595703, -0.011163163930177689, -0.058295391499996185, -0.04738866165280342, 0.0968175083398819, 0.003956100437790155, -0.19718754291534424, -0.18541550636291504, 0.20521993935108185], [0.17259284853935242, -0.038792915642261505, -0.5357918739318848, -0.08711950480937958, -0.11550875753164291, -0.2983752191066742, 0.3825080990791321, -0.05030028149485588, -0.14071813225746155, -0.1493523120880127, -0.5115503668785095, -0.2910654544830322, -0.17726784944534302, -0.3972175717353821, -0.02084570936858654, -0.42184850573539734, -0.21478162705898285, 0.28882941603660583, -0.428136944770813, -0.2237865924835205, -0.20838148891925812, -0.3660449981689453, -0.5643543004989624, -0.24970175325870514, 0.19966189563274384, -0.2649261951446533, -0.13842293620109558, -0.007597536779940128, -0.028809404000639915, 0.1253964751958847, -0.11267509311437607, 0.1774749457836151, -0.1841960847377777, 0.09148292988538742, -0.06887901574373245, -0.24457034468650818, -0.21986638009548187, -0.20621870458126068, -0.28817155957221985, -0.5478253364562988, -0.13448062539100647, -0.03973260521888733, -0.041053008288145065, -0.1693224310874939, 0.10062523186206818, 0.0534868948161602, 0.04036857560276985, -0.19561192393302917], [0.2204921692609787, -0.43367424607276917, 0.07585376501083374, -0.3433111011981964, 0.37653887271881104, -0.015187788754701614, -0.40171873569488525, 0.29494166374206543, -0.28073999285697937, -0.7131936550140381, -0.6027947068214417, 0.060180168598890305, 0.024118686094880104, 0.13823626935482025, -0.024006685242056847, -0.01534190122038126, -0.4567021131515503, 0.004499093629419804, -0.16064764559268951, -0.15489248931407928, -0.11330603063106537, 0.3252411484718323, -0.4028305411338806, 0.09160930663347244, -0.352236807346344, -0.3560006022453308, -0.205377459526062, -0.11706309020519257, -0.09298165142536163, -0.24556447565555573, 0.027209805324673653, -0.2635483741760254, -0.32452839612960815, -0.1150360256433487, -0.38004937767982483, -0.3104207515716553, -0.13142900168895721, -0.11983393877744675, -0.08410972356796265, -0.06119194254279137, -0.12400081008672714, -0.20894229412078857, -0.29599687457084656, 0.028641177341341972, -0.130461186170578, 0.04351309314370155, 0.09384208917617798, -0.07000146061182022], [-0.1967947781085968, 0.1240815743803978, 0.3259231150150299, 0.05670778453350067, -0.0249123927205801, -0.08200114220380783, -0.16805152595043182, -0.08734308183193207, -0.1974436640739441, 0.10714816302061081, -0.00877442117780447, -0.1788075864315033, 0.24511350691318512, -0.05780238285660744, -0.0024873479269444942, 0.12767858803272247, -0.008455958217382431, -0.4468262493610382, 0.2200748175382614, -0.11599122732877731, 0.06897736340761185, -0.2719511091709137, -0.36578038334846497, -0.11580272018909454, -0.5319390892982483, -0.1979631781578064, -0.18298576772212982, -0.02917405217885971, -0.26737216114997864, -0.19281421601772308, 0.005745837464928627, 0.08984255790710449, -0.02533549629151821, 0.059827469289302826, -0.4992958903312683, -0.2846269905567169, 0.20125770568847656, 0.10107531398534775, 0.12432651966810226, 0.0643286257982254, 0.12531568109989166, 0.16435059905052185, -0.10742077976465225, -0.05858481675386429, 0.038801323622465134, 0.09931857883930206, -0.05987503379583359, -0.17158833146095276], [-0.3085922598838806, -0.0946449562907219, -0.4513798952102661, -0.2361723780632019, 0.024676376953721046, 0.13803060352802277, -0.25895559787750244, -0.016249869018793106, -0.7488648891448975, -0.4781562387943268, -0.38018879294395447, -0.10152719914913177, 0.06033957377076149, -0.2239299863576889, -0.39631617069244385, -0.357521653175354, -0.48641112446784973, -0.09513882547616959, -0.031284261494874954, 0.00710637541487813, -0.30839890241622925, -0.1657780259847641, -0.18385328352451324, 0.06967975944280624, -0.373374342918396, -0.26112622022628784, -0.01330825686454773, -0.09562902897596359, 0.10395239293575287, -0.05848747491836548, -0.16867128014564514, -0.14752942323684692, -0.41435953974723816, -0.09740233421325684, 0.05360088124871254, -0.18462331593036652, -0.4090496599674225, -0.282558411359787, 0.09493304044008255, 0.06845895200967789, -0.1162257120013237, 0.21932221949100494, -0.024278994649648666, 0.003989521414041519, 0.019845306873321533, 0.07722871750593185, 0.0978773906826973, -0.13390521705150604], [0.03446657955646515, -0.42687222361564636, -0.47847676277160645, -0.31616294384002686, -0.010327798314392567, -0.37843912839889526, 0.21015460789203644, -0.004702847450971603, -0.78985196352005, -0.5969020128250122, -0.5547239780426025, -0.3123653531074524, -0.10042181611061096, -0.24189487099647522, -0.0032501474488526583, 0.058319710195064545, -0.21043133735656738, 0.41597720980644226, -0.20112526416778564, -0.01791318692266941, -0.08823952823877335, 0.12113320082426071, -0.03468575328588486, -0.037025950849056244, 0.5219398736953735, -0.41776496171951294, -0.2378089427947998, -0.5244635343551636, 0.12061499059200287, 0.4109678864479065, -0.6482459306716919, -0.16364166140556335, -0.49473559856414795, -0.3994991183280945, -0.48942282795906067, -0.43168991804122925, -0.1172771081328392, -0.16118189692497253, -0.16204647719860077, -0.2501271069049835, 0.11881878226995468, -0.3286897540092468, -0.06653910130262375, -0.021577294915914536, 0.20370806753635406, -0.46989905834198, -0.2500901222229004, 0.023613039404153824], [0.1416826844215393, -0.17493583261966705, 0.09611355513334274, -0.4182978570461273, 0.1925298422574997, -0.16310729086399078, -0.6276203393936157, -0.08396994322538376, -0.43505188822746277, -0.1507454812526703, -0.3159874379634857, -0.1811959147453308, 0.2538965344429016, -0.14030073583126068, -0.005113914608955383, -0.32090842723846436, -0.2557268440723419, 0.0488467700779438, -0.06361562758684158, -0.04831443354487419, -0.05702076107263565, 0.011559604667127132, -0.10694865882396698, -0.048231810331344604, -0.5133073925971985, -0.20896992087364197, -0.1291954070329666, -0.31552523374557495, 0.06321974843740463, -0.35338741540908813, -0.09359922260046005, -0.13706348836421967, -0.1405065357685089, 0.2007996141910553, -0.23641493916511536, 0.03938955068588257, -0.14848561584949493, -0.49870210886001587, -0.06152845546603203, -0.3183020353317261, -0.3032604455947876, -0.09719919413328171, -0.08262649923563004, 0.14048191905021667, -0.20307670533657074, -0.15584245324134827, -0.18513566255569458, -0.10513367503881454], [-0.06982723623514175, 0.04823402687907219, -0.4056735336780548, -0.5881633162498474, 0.10878236591815948, -0.007796215359121561, -0.13033057749271393, 0.010294967330992222, -0.11177751421928406, -0.3150058388710022, 0.6536308526992798, 0.23365910351276398, -0.1432814747095108, -0.09008588641881943, -0.5943435430526733, -0.0515473298728466, 0.030873529613018036, -0.06055023521184921, -0.29177069664001465, -0.24575765430927277, -0.3065415322780609, 0.008515401743352413, -0.17688655853271484, -0.28330063819885254, -0.24258741736412048, -0.2568870484828949, -0.12988968193531036, -0.23837494850158691, -0.12273751944303513, -0.3097163140773773, -0.023432720452547073, -0.004338914528489113, -0.25676342844963074, 0.15730369091033936, -0.06262436509132385, -0.21197111904621124, 0.16744285821914673, -0.08780594170093536, -0.04829345643520355, -0.250659316778183, -0.1411188840866089, -0.12358000129461288, -0.013058421202003956, -0.3295495808124542, 0.07911869138479233, 0.09418277442455292, -0.18449944257736206, -0.16260074079036713], [-0.09438209235668182, -0.3151717782020569, 0.3911430835723877, -0.12338966876268387, -0.30300021171569824, -0.38583558797836304, 0.10983827710151672, 0.06246449425816536, -0.26665061712265015, 0.030394110828638077, -0.35802021622657776, -0.20597313344478607, 0.013919135555624962, 0.3656127154827118, -0.21714797616004944, -0.09200989454984665, -0.1673438996076584, -0.07055193185806274, -0.17894819378852844, 0.04421538859605789, -0.1635112464427948, -0.31194868683815, 0.08644601702690125, -0.7378222346305847, -0.16652871668338776, 0.060959357768297195, 0.17407934367656708, -0.3861529231071472, -0.002729099942371249, -0.24553930759429932, 0.03605852276086807, -0.16362090408802032, -0.056658003479242325, -0.15189683437347412, -0.04880034551024437, -0.295773983001709, 0.0999365970492363, -0.35829225182533264, -0.06399864703416824, -0.3358577787876129, -0.28106698393821716, -0.014644655399024487, -0.17154261469841003, 0.06933778524398804, -0.16204480826854706, -0.025553004816174507, -0.03170113265514374, 0.1478787213563919], [-0.19179067015647888, 0.15521632134914398, -0.34713131189346313, 0.1266206055879593, 0.21499906480312347, 0.14707964658737183, -0.3890981674194336, -0.34235328435897827, -0.2634267210960388, -0.17897266149520874, -0.06367334723472595, -0.0930466502904892, -0.061285052448511124, -0.5263826847076416, 0.3265645503997803, 0.16352540254592896, 0.08664077520370483, -0.1338699609041214, -0.06507743149995804, -0.38063812255859375, 0.06668611615896225, -0.16807611286640167, -0.6536963582038879, 0.3077545762062073, 0.045071445405483246, 0.042558714747428894, -0.18820776045322418, -0.257891982793808, -0.11252138018608093, -0.16103650629520416, 0.08208116143941879, -0.058244943618774414, 0.18155789375305176, 0.093048095703125, -0.16797317564487457, -0.073288194835186, -0.23695479333400726, 0.0984887108206749, -0.29915210604667664, -0.11310131102800369, -0.057505808770656586, 0.10303469747304916, -0.06465345621109009, -0.1370900422334671, 0.22954663634300232, 0.12223757058382034, -0.4217832088470459, -0.1730354279279709], [-0.3149060606956482, 0.14302246272563934, -0.06392569839954376, 0.03717338293790817, 0.11269193142652512, -0.022500965744256973, -0.021223098039627075, -0.055357031524181366, 0.21275532245635986, -0.12185841053724289, 0.3525628447532654, -0.28484290838241577, -0.21442502737045288, -0.23721089959144592, -0.11405425518751144, -0.15977485477924347, 0.011688120663166046, -0.4265197813510895, -0.13531596958637238, -0.38033002614974976, 0.21177880465984344, -0.04597640782594681, -0.7519609928131104, -0.2226930409669876, -0.16885419189929962, 0.04206223040819168, -0.19390501081943512, 0.45137807726860046, -0.09791745245456696, -0.1563786119222641, 0.3010246157646179, -0.3262574374675751, 0.15488672256469727, -0.3329249918460846, -0.28820425271987915, -0.2799190580844879, -0.24784153699874878, -0.24322296679019928, -0.23325087130069733, 0.15943990647792816, -0.2490055114030838, 0.20724007487297058, -0.13280323147773743, -0.09472835063934326, -0.3178478479385376, -0.14367160201072693, -0.4640128016471863, -0.2121029496192932], [0.013928423635661602, -0.29243505001068115, -0.07245630025863647, -0.2878655195236206, -0.0754866674542427, -0.06547246128320694, -0.30971378087997437, 0.1746406853199005, -0.5203563570976257, -0.6507313251495361, -0.4745980501174927, -0.26128965616226196, -0.4399093687534332, -0.1710006594657898, 0.07891343533992767, 0.0865674689412117, 0.07953255623579025, -0.4938037395477295, -0.07852818816900253, -0.5813999176025391, -0.15953676402568817, -0.41507014632225037, -0.07295595854520798, 0.17614233493804932, -0.33369961380958557, -0.5614681243896484, -0.0130138099193573, -0.15326036512851715, 0.011837015859782696, -0.05602729320526123, -0.3894811272621155, -0.08955816924571991, -0.13656999170780182, 0.09194688498973846, 0.2704145014286041, -0.16715313494205475, -0.035235099494457245, 0.09366773068904877, 0.3428398370742798, -0.036544375121593475, -0.14164559543132782, -0.20745675265789032, 0.12391459196805954, -0.24032658338546753, -0.2985721826553345, 0.02777165360748768, -0.14102618396282196, 0.20051102340221405], [-0.035853903740644455, 0.0342850424349308, 0.19150905311107635, 0.051460523158311844, -0.16391465067863464, -0.24831748008728027, -0.6050145626068115, 0.0007741161971352994, 0.08454758673906326, -0.27328506112098694, -0.11050256341695786, -0.1736987829208374, 0.011081474833190441, 0.2663441002368927, -0.014400116167962551, 0.20349976420402527, -0.3749105930328369, -1.0662139654159546, 0.12855719029903412, -0.07470634579658508, -0.01398072112351656, -0.5775245428085327, -0.3010687232017517, -0.42789700627326965, -0.22087398171424866, 0.17743174731731415, -0.213721364736557, -0.2138921022415161, -0.175746887922287, -0.3476349413394928, -0.09586836397647858, -0.5382568836212158, 0.04468965530395508, -0.1200985461473465, -0.37529999017715454, -0.06512300670146942, -0.13210876286029816, -0.26942819356918335, 0.2728947103023529, -0.13483841717243195, 0.13253402709960938, 0.09545164555311203, -0.34028586745262146, -0.04238603636622429, -0.14822609722614288, -0.07026910781860352, -0.02437715046107769, -0.1531851887702942], [-0.11181936413049698, -0.16790585219860077, 0.2618456780910492, -0.35364535450935364, 0.0562342144548893, -0.04012700542807579, -0.3844759166240692, 0.25334954261779785, 0.07568443566560745, -0.08656903356313705, -0.3481251895427704, -0.27330225706100464, 0.15490365028381348, 0.2109537124633789, -0.12409589439630508, -0.1569240689277649, 0.07027842849493027, 0.08982603251934052, 0.29240113496780396, 0.03618776425719261, -0.18678341805934906, -0.2145959585905075, -0.25783124566078186, -0.059232428669929504, -0.14169903099536896, 0.17826518416404724, -0.028476238250732422, -0.0008363681845366955, -0.16689909994602203, -0.22015725076198578, 0.09287086874246597, 0.09525836259126663, 0.0016043772920966148, 0.2469351440668106, -0.12094208598136902, 0.1274249404668808, -0.04886431619524956, -0.16554959118366241, -0.12381482869386673, -0.11682216078042984, 0.2752598524093628, -0.0027546659111976624, -0.05400991439819336, -0.17668427526950836, -0.010088503360748291, -0.362073689699173, -0.21887628734111786, 0.02965301275253296], [-0.011688361875712872, -0.09450802952051163, 0.09050647914409637, -0.3275199234485626, -0.4249292314052582, -0.2700788080692291, -0.012108397670090199, -0.11778567731380463, -0.19104057550430298, -0.2504265010356903, -0.14932331442832947, 0.01873381994664669, 0.2922937273979187, 0.2190895527601242, -0.020582951605319977, -0.11063586920499802, -0.04199797660112381, -0.250401109457016, 0.007362188771367073, 0.511981189250946, -0.11005711555480957, -0.3169611990451813, 0.15239906311035156, -0.5261443853378296, -0.12674564123153687, -0.24321939051151276, -0.3969517946243286, -0.07595859467983246, -0.05980106070637703, -0.12040115892887115, -0.057446904480457306, -0.42161309719085693, -0.2990679144859314, -0.22274154424667358, -0.15266792476177216, -0.32998037338256836, -0.2032434046268463, 0.08275467157363892, 0.0081747742369771, -0.1627780795097351, 0.2106807827949524, -0.35512077808380127, -0.20990684628486633, 0.21388325095176697, 0.14397242665290833, -0.049053438007831573, 0.26902350783348083, -0.12163589894771576], [0.35639047622680664, -0.25533315539360046, 0.29026490449905396, -0.08825956284999847, -0.32321515679359436, -0.02545907534658909, -0.1539989709854126, 0.0737103521823883, -0.33250150084495544, -0.2297402322292328, -0.11799009144306183, -0.19814465939998627, 0.01672375202178955, 0.35672155022621155, -0.40832000970840454, -0.6440424919128418, -0.7147800326347351, -0.025619005784392357, -0.22515928745269775, -0.08318114280700684, -0.3060801029205322, -0.16593904793262482, -0.2251441925764084, -0.6709463000297546, -0.3614739775657654, -0.1154593676328659, -0.09244205802679062, -0.022850941866636276, -0.10220556706190109, -0.22661247849464417, -0.17217649519443512, -0.42791831493377686, 0.0362163707613945, -0.2662569284439087, -0.21974308788776398, -0.10557791590690613, -0.1242339015007019, -0.2162235975265503, -0.32628756761550903, -0.10901246219873428, -0.04803210496902466, 0.11591793596744537, -0.14305312931537628, -0.07366698235273361, 0.14318741858005524, 0.05877635255455971, -0.20878516137599945, 0.03428959473967552], [-0.2528228461742401, -0.21302519738674164, -0.3945656716823578, -0.19139398634433746, 0.2335115671157837, -0.017633873969316483, 0.15251418948173523, -0.8698132038116455, -0.19205638766288757, 0.364765465259552, -0.28225043416023254, -0.2511475086212158, -0.28469350934028625, -0.23829588294029236, -0.5938940048217773, -0.1171095073223114, -0.840203583240509, -0.05903720110654831, -0.21137364208698273, -0.035419754683971405, 0.08052221685647964, -0.5834636092185974, -0.311132550239563, -0.12086641788482666, 0.024162815883755684, 0.07046562433242798, -0.3307932913303375, 0.09393534809350967, -0.05244072526693344, -0.25384339690208435, 0.2639212906360626, -0.4791957437992096, 0.16750644147396088, -0.31021904945373535, -0.5340681672096252, 0.1438235640525818, -0.3342834711074829, -0.08000319451093674, -0.23229271173477173, -0.11764028668403625, -0.050796765834093094, -0.0639379620552063, -0.06730044633150101, -0.34615951776504517, -0.23007723689079285, -0.1588389128446579, -0.16984878480434418, -0.056391116231679916], [0.0237908735871315, -0.30709555745124817, 0.2321283221244812, 0.17015336453914642, -0.09055405110120773, 0.006406481843441725, -0.2230701595544815, -0.22114548087120056, -0.14243513345718384, -0.1646527796983719, -0.2461828738451004, -0.15215732157230377, 0.16705285012722015, 0.1292385458946228, 0.03512558341026306, -0.04681618511676788, -0.2626308500766754, -0.18276818096637726, 0.1061326265335083, -0.06514787673950195, -0.2259618192911148, -0.28229281306266785, 0.16716808080673218, -0.2680690586566925, -0.11835228651762009, -0.04097643122076988, -0.054616060107946396, -0.11168117076158524, -0.2693658173084259, -0.3099735677242279, -0.291521281003952, -0.35835057497024536, -0.1395052820444107, 0.22925035655498505, -0.12748399376869202, 0.0982232615351677, -0.2531532049179077, -0.23062095046043396, 0.014024901203811169, -0.08710189163684845, 0.018025215715169907, 0.004849508870393038, -0.09064845740795135, -0.19381849467754364, 0.06984388083219528, -0.3353094160556793, -0.030192289501428604, -0.05199536308646202], [-0.15395984053611755, -0.30236175656318665, -0.541959822177887, -0.12010471522808075, 0.23363126814365387, -0.271759033203125, -0.5308096408843994, 0.30225250124931335, -0.4915188252925873, -0.05857428163290024, -0.22082704305648804, -0.2776516079902649, -0.3961067497730255, -0.40856218338012695, 0.06535159796476364, 0.21659380197525024, 0.01072706375271082, 0.006397303659468889, -0.055168040096759796, 0.001317427260801196, 0.03192789480090141, 0.25007227063179016, -0.19682815670967102, 0.18267905712127686, -0.33279356360435486, -0.05146083980798721, -0.30517658591270447, -0.2533138394355774, -0.13857392966747284, 0.284606397151947, -0.09081326425075531, -0.08229077607393265, -0.22762925922870636, -0.24869801104068756, -0.25312361121177673, 0.16113540530204773, -0.13755708932876587, 0.37199774384498596, 0.0058137644082307816, 0.038237638771533966, -0.0485699400305748, -0.12824979424476624, 0.025626905262470245, 0.01264577079564333, -0.38283127546310425, 0.28882917761802673, 0.14133009314537048, 0.004209481179714203], [0.03399872034788132, -0.03231959417462349, 0.0030707702971994877, -0.48697564005851746, -0.09058809280395508, -0.06725671142339706, -0.3601091802120209, -0.21790318191051483, -0.20771180093288422, -0.2699449956417084, -0.4733116328716278, -0.010639439336955547, -0.4140118956565857, -0.41610151529312134, 0.033957090228796005, 0.23258911073207855, 0.1491488814353943, -0.3473545014858246, 0.10506559163331985, -0.018919311463832855, 0.13886728882789612, -0.5523433089256287, -0.20983533561229706, -0.2344164252281189, -0.3060038387775421, -0.6701962351799011, 0.4250902235507965, -0.25330284237861633, -0.2029293030500412, -0.3754066824913025, 0.1349254995584488, 0.002325191628187895, -0.1395215392112732, 0.42422667145729065, -0.23981980979442596, 0.3689314126968384, -0.33407074213027954, -0.038204774260520935, 0.22698144614696503, 0.1571228802204132, -0.09434240311384201, 0.11404193937778473, 0.15451650321483612, -0.18796226382255554, -0.1717483550310135, -0.0811975821852684, 0.1150076687335968, 0.020049618557095528], [0.10013090074062347, -0.44876861572265625, -0.17460569739341736, -0.16438212990760803, 0.1666589081287384, -0.17442092299461365, -0.26009225845336914, -0.14299188554286957, -0.4675734043121338, 0.42295077443122864, -0.5792596340179443, -0.18201011419296265, -0.5390167236328125, 0.13334684073925018, -0.25735318660736084, -0.2474660873413086, -0.04628528654575348, 0.0977182611823082, -0.22271813452243805, -0.07086355984210968, -0.49666082859039307, 0.09493444114923477, -0.0971907377243042, -0.2397298961877823, -0.42707890272140503, 0.3469409942626953, -0.34016153216362, -0.3212657868862152, 0.11055724322795868, -0.6455060839653015, -0.4772612750530243, -0.2601158916950226, -0.17977796494960785, -0.5166701078414917, -0.005535169504582882, -0.1802186816930771, 0.22097808122634888, -0.01300845667719841, -0.3664872944355011, -0.08834636211395264, 0.005430738441646099, 0.18977387249469757, -0.2355773001909256, 0.14730966091156006, -0.08557569980621338, 0.2052103877067566, -0.08216574788093567, -0.1690661907196045], [0.14879129827022552, -0.31244784593582153, -0.17528191208839417, -0.24401900172233582, 0.07050256431102753, -0.09968636929988861, -0.36074569821357727, -0.2797483503818512, -0.27235785126686096, -0.6272726058959961, -0.04416830465197563, 0.01523043867200613, -0.24156786501407623, -0.2872120141983032, 0.407354474067688, 0.12918969988822937, -0.263778418302536, -0.07174190133810043, -0.4423869550228119, -0.23575419187545776, -0.30931633710861206, -0.1728406548500061, 0.07147704064846039, -0.007308421190828085, -0.28496477007865906, -0.5577720999717712, 0.19064690172672272, 0.07184281200170517, -0.08535626530647278, -0.15210966765880585, -0.32577455043792725, 0.11363349854946136, -0.07822178304195404, -0.15963295102119446, -0.1054520383477211, -0.2954118847846985, -0.25523841381073, -0.24256478250026703, 0.18846730887889862, -0.30406293272972107, -0.17590072751045227, 0.1772078424692154, 0.01176945585757494, -0.05129282549023628, -0.11443883180618286, -0.13808120787143707, 0.24282629787921906, 0.08909358084201813], [-0.09098533540964127, -1.071575403213501, -0.7129441499710083, -0.9760159254074097, -1.255293846130371, 0.04474688321352005, -0.13927093148231506, -0.16314658522605896, -0.48538264632225037, 0.01123865321278572, -0.7068425416946411, 0.0018610465340316296, -0.36481720209121704, 0.7744525074958801, -1.0162101984024048, -1.121016025543213, -0.5006440281867981, -0.48129868507385254, -0.5591248273849487, -0.44352394342422485, -0.7665460109710693, 0.026388440281152725, 0.8098019957542419, -0.4173000752925873, -0.27965953946113586, 0.0634964108467102, -0.8867253065109253, -0.3437361419200897, -0.26543885469436646, -0.3268173635005951, -0.3713434636592865, -0.5637165307998657, 0.01919572800397873, 0.030457358807325363, -0.30433598160743713, -0.5775658488273621, -0.300763875246048, -0.050426334142684937, -0.6510923504829407, -0.5211742520332336, -0.5415245294570923, -0.33983856439590454, 0.05699100345373154, -0.08985776454210281, -0.3524223566055298, 0.04155119135975838, -0.11219407618045807, -0.12306056916713715], [-0.06095355004072189, -0.17486494779586792, 0.23182065784931183, -0.3473471999168396, -0.4034671485424042, 0.032194457948207855, -0.2676564157009125, 0.1967775970697403, -0.06548359990119934, -0.06672179698944092, -0.2932513952255249, -0.1939297765493393, 0.16208018362522125, -0.16311760246753693, -0.10165517032146454, -0.2926073968410492, -0.1107519119977951, -0.19819864630699158, 0.12223596125841141, 0.2184283584356308, -0.019649721682071686, 0.17033173143863678, -0.1120096743106842, -0.3426319658756256, 0.10117242485284805, 0.09342218190431595, 0.02725861221551895, -0.25620144605636597, 0.1422645002603531, -0.5766586661338806, -0.07924097776412964, 0.25010842084884644, -0.015043320134282112, -0.09852300584316254, -0.0018696107435971498, -0.1442715972661972, -0.26763829588890076, 0.059591010212898254, -0.19289365410804749, 0.04792172089219093, 0.01639620214700699, -0.3610488176345825, -0.07228608429431915, -0.3309849798679352, 0.10893180966377258, -0.040766842663288116, -0.11188425868749619, -0.08731937408447266], [0.021251792088150978, -0.022391125559806824, 0.07289661467075348, 0.07039472460746765, -0.1853155791759491, 0.0405251644551754, -0.16985514760017395, -0.12723015248775482, -0.16232766211032867, -0.08002691715955734, -0.042448606342077255, 0.10233956575393677, -0.03591714799404144, -0.19982047379016876, -0.0990649163722992, -0.03175938501954079, -0.12882199883460999, -0.1052936390042305, -0.10228852927684784, -0.15678654611110687, -0.11424067616462708, -0.10669293999671936, -0.08372306823730469, -0.0838153213262558, -0.11114050447940826, -0.05231677368283272, 0.22148633003234863, 0.08666092157363892, 0.2655538022518158, -0.04141160845756531, 0.030644670128822327, -0.34791287779808044, -0.39738887548446655, -0.03815310448408127, -0.07417560368776321, -0.18865028023719788, -0.0064392476342618465, -0.06570553779602051, -0.018236517906188965, -0.15064482390880585, 0.260660320520401, 0.08312732726335526, -0.30463624000549316, -0.24879905581474304, -0.18176016211509705, -0.24566885828971863, -0.015509472228586674, -0.2937827408313751], [-0.23423632979393005, -0.17389816045761108, 0.02332219108939171, 0.03150707110762596, -0.029655486345291138, 0.0808691531419754, -0.31954604387283325, -0.018399275839328766, -0.0986972525715828, -0.5203760862350464, -0.448618620634079, -0.2778618037700653, -0.023270871490240097, -0.022536378353834152, -0.08007079362869263, -0.2188979834318161, -0.30326205492019653, 0.03839029744267464, -0.06406731903553009, -0.3730471432209015, -0.2674117088317871, -0.04449780657887459, 0.0895870178937912, -0.11687716096639633, -0.31614482402801514, -0.2662695050239563, -0.10125134885311127, -0.47135743498802185, 0.11686410009860992, -0.2485496550798416, 0.20751069486141205, -0.014033971354365349, -0.20159782469272614, 0.11632229387760162, -0.3934860825538635, -0.15240266919136047, -0.16735559701919556, -0.2989269495010376, -0.27303609251976013, -0.2801167964935303, -0.2642826437950134, 0.024740273132920265, -0.18397268652915955, -0.11787082999944687, 0.11541043221950531, 0.15350157022476196, -0.10289761424064636, 0.06107578054070473], [-0.016221817582845688, -0.11038615554571152, -0.12684129178524017, -0.3273932933807373, -0.2489166110754013, -0.1855892688035965, -0.1669953465461731, 0.06978505849838257, 0.13720068335533142, -0.3646470010280609, -0.22537891566753387, -0.15715017914772034, -0.00227720127440989, 0.05893956869840622, -0.18390320241451263, 0.09747850149869919, 0.07866009324789047, -0.07060813158750534, 0.09312525391578674, -0.036167703568935394, -0.006172983441501856, -0.08941640704870224, -0.11798311769962311, 0.08001774549484253, -0.44460222125053406, -0.1490936428308487, 0.049209482967853546, -0.05177909880876541, 0.01815083995461464, -0.20635485649108887, 0.038112569600343704, 0.20839032530784607, -0.30052846670150757, -0.3643379807472229, -0.13371223211288452, 0.1281435489654541, -0.11998795717954636, -0.005497846752405167, 0.17663249373435974, -0.3596695065498352, 0.21101783215999603, 0.3205769956111908, 0.014251324348151684, 0.005299566313624382, -0.12907059490680695, 0.08149168640375137, -0.03975401818752289, 0.14571747183799744], [-0.17989709973335266, 0.03754144161939621, 0.08606867492198944, -0.5381828546524048, 0.34039872884750366, -0.13169406354427338, -0.2530670464038849, -0.4230457544326782, -0.3701207935810089, 0.11906169354915619, -0.3702259957790375, -0.3547547161579132, 0.38140714168548584, -0.047900259494781494, -0.18301625549793243, -0.31896281242370605, -0.1823958307504654, -0.20273934304714203, -0.15090665221214294, -0.36098968982696533, -0.2880708575248718, 0.2570241391658783, -0.22511115670204163, 0.018670789897441864, -0.18296942114830017, -0.18004123866558075, -0.03822483867406845, -0.115167036652565, -0.14243347942829132, -0.18907639384269714, -0.29324325919151306, 0.08524584025144577, 0.16287784278392792, -0.4577312767505646, -0.0074342358857393265, -0.1094086542725563, 0.010027564130723476, -0.0803513377904892, -0.3130109906196594, 0.11038854718208313, -0.35558021068573, -0.07444556057453156, -0.22451986372470856, -0.18496470153331757, -0.12589474022388458, -0.2864748239517212, 0.33389902114868164, -0.07687303423881531], [-0.04561754688620567, -0.6955058574676514, -0.6571003198623657, 0.0849948525428772, -0.06621638685464859, 0.01600465178489685, 0.17890159785747528, 0.046298276633024216, -0.23276568949222565, -0.4110724627971649, -0.15208567678928375, -0.21959076821804047, -0.29805999994277954, -0.029456090182065964, -0.20357994735240936, 0.06714918464422226, -0.2091124802827835, 0.33119311928749084, -0.2156118005514145, -0.34759294986724854, -0.23725612461566925, 0.1365811973810196, -0.00022258971875999123, -0.015418516471982002, 0.21221032738685608, -0.3430899977684021, -0.17049220204353333, -0.13506515324115753, -0.32038527727127075, 0.1561860293149948, -0.6830877661705017, -0.046954695135354996, -0.21560633182525635, -0.15810099244117737, -0.03262936323881149, -0.04994715377688408, 0.032762832939624786, -0.09553297609090805, 0.021350858733057976, -0.31713545322418213, -0.18999908864498138, -0.41964346170425415, -0.42328235507011414, 0.4829927980899811, -0.09617914259433746, -0.048566292971372604, -0.29669320583343506, -0.03210211917757988], [-0.13391448557376862, -0.16330792009830475, -0.4726591110229492, -0.07963446527719498, -0.28940171003341675, -0.26426559686660767, -0.1440994143486023, 0.1738751083612442, 0.04845215752720833, -0.40213534235954285, -0.1790885031223297, -0.17881348729133606, 0.015207293443381786, -0.40653231739997864, 0.1500246822834015, 0.3117590844631195, -0.3265216648578644, -0.3668706715106964, 0.3219614624977112, -0.14915820956230164, -0.18209566175937653, 0.0642223209142685, -0.32680845260620117, -0.7282451391220093, -0.6509053111076355, -0.09134455770254135, -0.4413618743419647, -0.08901143819093704, -0.3491262197494507, -0.3481430411338806, 0.046814050525426865, 0.012113937176764011, -0.2990301847457886, -0.1799248456954956, -0.05186106264591217, -0.04829459637403488, -0.24648180603981018, -0.18099984526634216, 0.02687103860080242, -0.04067380353808403, -0.0313090980052948, -0.06475318223237991, 0.12768198549747467, -0.08635977655649185, -0.056029271334409714, -0.03821203485131264, -0.07467560470104218, -0.30838698148727417], [0.04717328026890755, -0.4800738990306854, 0.0007552787428721786, -0.37190788984298706, -0.023813582956790924, -0.16677971184253693, -0.1469392627477646, -0.190106600522995, -0.11704704910516739, -0.7308118939399719, -0.15478025376796722, -0.040205370634794235, -0.04649166017770767, -0.3345690369606018, -0.4273688495159149, -0.3949647545814514, -0.2853197455406189, -0.13117210566997528, -0.6187892556190491, -0.27171939611434937, -0.3736325800418854, -0.29979076981544495, -0.5919339656829834, -0.4402114748954773, 0.08639374375343323, -0.4416004717350006, 0.0040225256234407425, -0.1986275464296341, -0.08370231091976166, -0.0015265758847817779, -0.26151177287101746, -0.1935587078332901, -0.4420816898345947, -0.0555240772664547, -0.698427140712738, 0.04876316711306572, -0.02315344475209713, 0.024278171360492706, -0.04814549908041954, -0.008237590081989765, -0.10837631672620773, -0.10074878484010696, 0.056782662868499756, 0.032300882041454315, -0.1174607127904892, -0.032935477793216705, 0.023440122604370117, -0.1457173079252243], [-0.06634344160556793, -0.1816200166940689, -0.404017835855484, -0.28961431980133057, 0.35430607199668884, -0.14378508925437927, -0.34825482964515686, -0.1959635466337204, -0.2146451324224472, -0.27385082840919495, -0.4304867088794708, 0.053102247416973114, -0.21230681240558624, -0.513426661491394, -0.22782820463180542, -0.45414289832115173, 0.05645476654171944, -0.08050218224525452, -0.2155950963497162, 0.037638112902641296, 0.08474978059530258, -0.061634257435798645, -0.18394948542118073, -0.24473796784877777, 0.2156469225883484, -0.05261356383562088, -0.18400165438652039, -0.06930048763751984, -0.30861327052116394, -0.28347915410995483, 0.0180733073502779, -0.36234864592552185, 0.01634690724313259, -0.012039865367114544, -0.17231610417366028, 0.06076150760054588, 0.29184848070144653, -0.034943852573633194, 0.18538467586040497, -0.17733317613601685, -0.1724727749824524, 0.17004422843456268, -0.07509922236204147, -0.17371776700019836, 0.19154898822307587, 0.015338010154664516, -0.003895172383636236, -0.02297833003103733], [-0.041884977370500565, -0.1074904277920723, 0.22720815241336823, -0.09603335708379745, 0.028751056641340256, -0.3159947395324707, -0.16086886823177338, 0.07798578590154648, -0.09916611760854721, -0.25473496317863464, 0.14010539650917053, -0.02868024818599224, -0.1388944536447525, -0.08327292650938034, -0.32853826880455017, -0.06726428121328354, -0.30152949690818787, 0.1522401124238968, 0.16481107473373413, -0.174801766872406, 0.26603975892066956, -0.06794143468141556, 0.001050401944667101, 0.028551986441016197, 0.002766885794699192, -0.07956523448228836, -0.11231563240289688, -0.0933682918548584, 0.006317258812487125, 0.1389191448688507, 0.05649302527308464, 0.06451080739498138, 0.09851935505867004, 0.009965524077415466, 0.188730388879776, 0.024070974439382553, -0.054737385362386703, 0.11273328214883804, -0.25601258873939514, -0.12077660113573074, 0.09160532057285309, 0.026367241516709328, -0.13905902206897736, -0.11288267374038696, -0.12269044667482376, -0.10836745798587799, 0.11421816796064377, -0.03480402007699013], [-0.30492570996284485, -0.13289129734039307, -0.2782152593135834, 0.17514431476593018, -0.13053414225578308, -0.16010324656963348, -0.2744165062904358, 0.33509790897369385, 0.016760608181357384, -0.22162394225597382, -0.09076067060232162, 0.34119176864624023, -0.06723066419363022, -0.24125178158283234, 0.05024271458387375, -0.3892643451690674, -0.034801822155714035, -0.1144927516579628, -0.22737330198287964, 0.026238610967993736, -0.1586901843547821, 0.04775983467698097, -0.08486174046993256, -0.20548436045646667, -0.09527350217103958, -0.04058663919568062, -0.0649031549692154, 0.0579058863222599, 0.05660922825336456, -0.07683708518743515, 0.009517270140349865, -0.2914283573627472, 0.09395746141672134, -0.20525161921977997, -0.10419445484876633, -0.03021666407585144, 0.14393086731433868, -0.46062132716178894, -0.4597110152244568, -0.12140905112028122, -0.14323946833610535, 0.01772044599056244, 0.1565559059381485, -0.04831713065505028, 0.11210445314645767, -0.08457832038402557, -0.029148530215024948, 0.06685725599527359], [0.2043399065732956, -0.08011704683303833, -0.2612735629081726, -0.27265727519989014, 0.025608563795685768, 0.12156295031309128, -0.0935167446732521, -0.23122954368591309, -0.08173180371522903, -0.3112243115901947, -0.12766319513320923, -0.20626267790794373, 0.2558489441871643, 0.12123271077871323, 0.0018783209379762411, -0.36957430839538574, -0.04536348581314087, -0.20932422578334808, 0.17961156368255615, -0.20736034214496613, 0.3235245645046234, -0.08167162537574768, -0.25320759415626526, 0.10756350308656693, -0.17004583775997162, 0.013135003857314587, 0.07018712162971497, -0.2410796731710434, -0.22675630450248718, 0.022088561207056046, 0.22035279870033264, -0.16032472252845764, -0.1942731887102127, -0.1161218211054802, 0.0864257737994194, -0.1825794279575348, 0.13880227506160736, -0.3168089687824249, -0.34117791056632996, 0.01613006927073002, 0.15365229547023773, -0.09709452837705612, 0.04640638455748558, -0.022486334666609764, -0.18518470227718353, -0.04247752204537392, -0.13355228304862976, -0.11984388530254364], [0.1179950013756752, -0.22536729276180267, -0.040665365755558014, 0.03280007466673851, 0.3087138831615448, 0.22840958833694458, -0.25407978892326355, -0.3592909276485443, 0.2660171389579773, 0.24821984767913818, 0.06464580446481705, -0.17626793682575226, -0.5188297033309937, -0.3131212294101715, -0.23700349032878876, 0.09845779836177826, -0.49732837080955505, -0.18632911145687103, -0.19981959462165833, -0.16768690943717957, -0.05466654151678085, -0.1289154291152954, -0.10140109062194824, -0.46253371238708496, 0.40663254261016846, 0.09499491751194, 0.11424493044614792, -0.3064183294773102, -0.08800027519464493, 0.02944478578865528, 0.20502327382564545, -0.11661358922719955, 0.2649683952331543, -0.11323440074920654, -0.25377511978149414, 0.19419746100902557, -0.08517230302095413, -0.4758757948875427, -0.17742526531219482, 0.30148404836654663, -0.08072562515735626, -0.23017261922359467, -0.10781461745500565, 0.035163238644599915, 0.0034343961160629988, 0.14974449574947357, 0.22478966414928436, -0.1124582514166832], [-0.22404104471206665, -0.6233461499214172, -0.0065744733437895775, -0.17059417068958282, -0.21098439395427704, -0.021183129400014877, -0.13906364142894745, -0.0553136020898819, -0.02649746462702751, -0.03799157589673996, -0.12258141487836838, -0.18186768889427185, 0.27780061960220337, 0.2873512804508209, -0.2651720643043518, -0.43714287877082825, -0.31860116124153137, -0.3862699866294861, -0.3823208808898926, 0.14049074053764343, -0.010217100381851196, -0.010315267369151115, 0.23082110285758972, -0.08908297121524811, -0.37991297245025635, -0.2988170385360718, -0.15144473314285278, -0.35618382692337036, -0.03263900801539421, -0.21296106278896332, -0.12089469283819199, -0.20557951927185059, -0.16263630986213684, -0.054411862045526505, 0.19962920248508453, -0.26704180240631104, -0.25269225239753723, -0.09984677284955978, -0.20989426970481873, -0.11033125966787338, -0.7888348698616028, -0.06690426170825958, -0.24159486591815948, -0.07258767634630203, -0.02754407376050949, -0.05856573209166527, 0.16875691711902618, 0.0009015722316689789], [0.02472652494907379, -0.2003425657749176, 0.10285454243421555, -0.08675981312990189, -0.08548741787672043, -0.11743747442960739, -0.0840112641453743, -0.20189885795116425, 0.20431332290172577, -0.14904911816120148, -0.1359587460756302, -0.07435354590415955, 0.003537064418196678, -0.2621627449989319, -0.17017827928066254, -0.009112260304391384, 0.009245069697499275, 0.20992743968963623, -0.18802884221076965, 0.07702631503343582, 0.03992915526032448, -0.1849214732646942, -0.2604827880859375, -0.009952891618013382, -0.2254943698644638, -0.1922362595796585, -0.06632031500339508, 0.04292592033743858, 0.13599102199077606, 0.03866373002529144, -0.3940461277961731, -0.46054360270500183, 0.2579638957977295, 0.1961999088525772, -0.13590039312839508, -0.3051068186759949, -0.01824183575809002, -0.10616685450077057, 0.10372655838727951, 0.11711445450782776, -0.02895335480570793, -0.29229140281677246, -0.1720607429742813, -0.01645205169916153, 0.05266375467181206, -0.07945651561021805, -0.034704647958278656, 0.026243938133120537], [-0.023804539814591408, -0.25923097133636475, 0.0018167003290727735, 0.07487870007753372, -0.22564265131950378, 0.08645530045032501, -0.025765251368284225, -0.07319023460149765, -0.19497595727443695, -0.34724950790405273, 0.2073489874601364, -0.09825097024440765, -0.10144229233264923, -0.018299799412488937, -0.2753365933895111, -0.2545534074306488, 0.02707507275044918, -0.48720255494117737, -0.22121910750865936, -0.02880275808274746, -0.07206185907125473, -0.3768746852874756, -0.26168614625930786, 0.03384878486394882, -0.14966237545013428, -0.08987683802843094, -0.1871248036623001, -0.033775199204683304, -0.10688649863004684, -0.2823505103588104, 0.13710935413837433, -0.038931846618652344, 0.07361768931150436, -0.1969916969537735, -0.09392041712999344, -0.0680401474237442, -0.02618739753961563, -0.20968608558177948, -0.12224521487951279, 0.039377469569444656, 0.07278643548488617, -0.06826832890510559, -0.25913625955581665, 0.15351754426956177, 0.06440533697605133, -0.23492048680782318, -0.16387616097927094, -0.06147463992238045], [-0.11035727709531784, -0.202407106757164, -0.144539013504982, -0.27223408222198486, -0.18512926995754242, -0.2905762791633606, 0.09188809245824814, -0.011947186663746834, -0.19961978495121002, 0.03404493257403374, 0.0017349495319649577, 0.03732448071241379, -0.1700330227613449, -0.18404339253902435, 0.02988475374877453, -0.013556309044361115, 0.009881741367280483, 0.0034062748309224844, -0.009804625064134598, -0.4854903817176819, -0.03505967929959297, -0.13856683671474457, -0.21948014199733734, -0.2186311036348343, -0.1710234433412552, -0.028511574491858482, -0.05257853865623474, -0.014551322907209396, -0.09273795038461685, -0.4036637842655182, 0.0195196233689785, 0.06805262714624405, -0.0007971556042321026, 0.0898618996143341, -0.20858816802501678, 0.07158925384283066, -0.07467645406723022, -0.38094067573547363, 0.14079149067401886, 0.06575465202331543, 0.08555367588996887, -0.18736182153224945, -0.1915503293275833, -0.11482102423906326, -0.19391675293445587, 0.07470937818288803, -0.038513828068971634, 0.011499923653900623], [-0.02078598365187645, -0.16368766129016876, -0.15875333547592163, -0.02904745191335678, 0.23118597269058228, 0.07018925994634628, 0.06260298937559128, -0.12928979098796844, 0.01951977051794529, -0.03980540856719017, 0.08496101200580597, 0.010168025270104408, 0.17771321535110474, -0.20230898261070251, 0.07037295401096344, -0.1445726752281189, -0.08532644063234329, -0.18208812177181244, 0.27789121866226196, 0.055638495832681656, -0.184267058968544, -0.26984015107154846, -0.2069399207830429, -0.030212296172976494, 0.2434956431388855, -0.16758796572685242, -0.31647107005119324, -0.14671659469604492, 0.10546540468931198, -0.12247002124786377, -0.0800023153424263, -0.13402439653873444, 0.022938871756196022, -0.14678996801376343, -0.14732389152050018, -0.02613252028822899, 0.093162402510643, -0.06106980890035629, -0.17591269314289093, -0.3055519163608551, -0.16109251976013184, -0.09922224283218384, -0.13842102885246277, -0.18080322444438934, -0.28185248374938965, 0.15477529168128967, 0.04967499151825905, 0.14363962411880493], [-0.14759142696857452, 0.14009834825992584, -0.17632384598255157, -0.05032287910580635, -0.14375510811805725, 0.047964394092559814, 0.007751136552542448, 0.1395256668329239, 0.06779298931360245, -0.14738774299621582, -0.18545721471309662, 0.19107885658740997, -0.13567331433296204, 0.024334929883480072, -0.0005719867767766118, -0.12469172477722168, -0.31493079662323, 0.08905822038650513, -0.12463701516389847, -0.09222617000341415, -0.21124106645584106, -0.18952712416648865, -0.33582863211631775, -0.07215393334627151, -0.13078317046165466, -0.17862790822982788, -0.10047438740730286, -0.21697357296943665, -0.03967805579304695, -0.1321181058883667, 0.14077599346637726, 0.014532412402331829, 0.27373063564300537, -0.06150055676698685, -0.0356123223900795, -0.13723699748516083, 0.1117083728313446, 0.11904997378587723, 0.05754394084215164, 0.07128055393695831, -0.2703075110912323, 0.00355548900552094, -0.09754248708486557, 0.09099306911230087, -0.20686371624469757, -0.31138715147972107, 0.025228211656212807, -0.35351845622062683], [-0.07169203460216522, -0.14245712757110596, 0.09811526536941528, -0.07152712345123291, -0.10780107975006104, 0.0736692026257515, -0.07169920951128006, 0.005580900236964226, 0.20155322551727295, 0.19936174154281616, -0.12108149379491806, 0.03690529614686966, -0.3729511499404907, -0.05025245621800423, -0.289232462644577, -0.20156748592853546, -0.07189308851957321, -0.06755898147821426, -0.044923316687345505, -0.057298582047224045, 0.015069674700498581, -0.18495407700538635, -0.24978044629096985, -0.02515629678964615, -0.07258831709623337, -0.01580662466585636, 0.069028340280056, -0.23839125037193298, -0.2487560212612152, -0.10588951408863068, -0.13760200142860413, -0.04016648977994919, -0.34655627608299255, -0.3182728588581085, -0.1643456071615219, -0.2111360728740692, 0.0615396574139595, -0.009702315554022789, 0.012725411914288998, -0.16738393902778625, 0.0016225515864789486, -0.2030877023935318, 0.012280113995075226, -0.0449662022292614, -0.022103097289800644, -0.09198228269815445, 0.1177651509642601, 0.21261750161647797], [0.10259036719799042, -0.05428619682788849, -0.02687489427626133, 0.14076921343803406, -0.49977537989616394, 0.24959686398506165, -0.04587065428495407, -0.011133323423564434, -0.03175179660320282, 0.15075461566448212, -0.030289674177765846, 0.0654895156621933, 0.09625665098428726, -0.1343643069267273, -0.20827479660511017, -0.25881701707839966, -0.19172850251197815, 0.03461576998233795, -0.12672658264636993, -0.15524046123027802, 0.27101999521255493, -0.0022877631708979607, 0.01757940836250782, -0.03474360331892967, 0.10981475561857224, -0.24030452966690063, -0.05637720599770546, 0.009963585063815117, 0.004455768037587404, 0.10435643792152405, 0.10112891346216202, 0.04977589473128319, -0.1020367220044136, -0.2951613664627075, -0.0986667349934578, 0.18701349198818207, -0.04562751203775406, -0.19897884130477905, 0.16792428493499756, 0.18555577099323273, -0.21474580466747284, -0.018973905593156815, -0.018330559134483337, -0.1235269159078598, -0.06800643354654312, 0.19417092204093933, -0.12310484051704407, -0.24627408385276794]]}],\n",
              "                        {\"coloraxis\": {\"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"height\": 750, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"width\": 750, \"xaxis\": {\"constrain\": \"domain\", \"dtick\": 1, \"scaleanchor\": \"y\"}, \"yaxis\": {\"autorange\": \"reversed\", \"constrain\": \"domain\", \"dtick\": 1}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8caa76cc-44e2-4b93-b2a8-62a6b446bde1');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'FIXN': 1, 'VACT': 2, 'NCMN': 3, 'PUNC': 4, 'CFQC': 5, 'DONM': 6, 'JCRG': 7, 'NCNM': 8, 'CNIT': 9, 'NPRP': 10, 'NTTL': 11, 'XVAM': 12, 'VSTA': 13, 'RPRE': 14, 'ADVN': 15, 'JSBR': 16, 'DDAC': 17, 'XVBM': 18, 'XVMM': 19, 'DIBQ': 20, 'PREL': 21, 'VATT': 22, 'XVAE': 23, 'DCNM': 24, 'CMTR': 25, 'FIXV': 26, 'PPRS': 27, 'XVBB': 28, 'DIAC': 29, 'PDMN': 30, 'DDAN': 31, 'CLTV': 32, 'ADVP': 33, 'NLBL': 34, 'ADVI': 35, 'CMTR@PUNC': 36, 'JCMP': 37, 'ADVS': 38, 'DDBQ': 39, 'NEG': 40, 'PNTR': 41, 'EITT': 42, 'DDAQ': 43, 'NONM': 44, 'EAFF': 45, 'DIAQ': 46, 'CVBL': 47}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k__Coy-HgPmM",
        "outputId": "16d50b07-4ad9-4494-991d-2f08a08a1c4f"
      },
      "source": [
        "# INSERT YOUR CODE HERE IF NEEDED\n",
        "print(\"transition matrix ที่แสดงออกมาจะเป็นการแสดงการเปลี่ยนจาก pos ของคำในปัจจุบันใน(แกน y) ไปยัง pos ของคำต่อไป(แกน x)\")\n",
        "print(\"โดย transition matrix ที่ได้ออกมานั้นมีค่าใกล้เคียงกับความเป็นจริง\\nตัวอย่างเช่น\")\n",
        "print(\"x=22,y=26 \\n   : FIXV -> VATT ซึ่งมีค่า\",transition_matrix[26][22],\"เป็นเหตุเป็นผลไปกับการที่ FIXVที่เป็นคำนำหน้าคำคุณศัพท์->VATTคำคุณศัพท์\\n    อย่าง/FXIV->เร็ว/VATT\")\n",
        "print(\"x=10,y=11 \\n   : NTTL -> NPRP ซึ่งมีค่า\",transition_matrix[11][10],\"เป็นเหตุเป็นผลไปกับการที่ NTTLที่เป็นการสื่อถึงคำนำหน้าชื่อของบุคคล->NPRPที่หมายถึงนามเฉพาะซึ่งรวมถึงชื่อบุคคล\\n    นาย/NTTL->พลวัต/NPRP\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transition matrix ที่แสดงออกมาจะเป็นการแสดงการเปลี่ยนจาก pos ของคำในปัจจุบันใน(แกน y) ไปยัง pos ของคำต่อไป(แกน x)\n",
            "โดย transition matrix ที่ได้ออกมานั้นมีค่าใกล้เคียงกับความเป็นจริง\n",
            "ตัวอย่างเช่น\n",
            "x=22,y=26 \n",
            "   : FIXV -> VATT ซึ่งมีค่า 0.809802 เป็นเหตุเป็นผลไปกับการที่ FIXVที่เป็นคำนำหน้าคำคุณศัพท์->VATTคำคุณศัพท์\n",
            "    อย่าง/FXIV->เร็ว/VATT\n",
            "x=10,y=11 \n",
            "   : NTTL -> NPRP ซึ่งมีค่า 0.65363085 เป็นเหตุเป็นผลไปกับการที่ NTTLที่เป็นการสื่อถึงคำนำหน้าชื่อของบุคคล->NPRPที่หมายถึงนามเฉพาะซึ่งรวมถึงชื่อบุคคล\n",
            "    นาย/NTTL->พลวัต/NPRP\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}